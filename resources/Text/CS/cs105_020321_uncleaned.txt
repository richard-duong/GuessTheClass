00:00

about Quiz three, this problem.

00:02

And I told you that I will look through

00:06

quizzes in see how you did on it.

00:08

And then I'll decide how much to go into this course.

00:11

And then I noticed

00:16

this submission should reveal the author's name.

00:19

No.

00:27

Okay. Exactly.

00:31

When we IV they wanted so I am asking okay.

00:39

It was pretty well done actually is an excellent, Right?

00:48

So I decided I will be

00:52

lazy and I will use the solution

00:55

because it was just excellent.

00:56

It was excellent response. Thank you.

00:59

Okay. Thank you, Emily. It was very good.

01:02

So we have a number of people surveyed.

01:10

They were asked about their age,

01:14

whether they can swim or not.

01:16

We're trying to test right here or whether there is

01:19

any connection between the age

01:23

and weather person can swim.

01:26

So our hypothesis, null hypothesis,

01:28

that there is no relationship between

01:32

the age and the ability to swing our null hypothesis,

01:35

the reason no relation.

01:40

Let's see. If we can disprove.

01:42

Alright, the data's right here,

01:50

40 to forty eight,

01:53

twenty eight, twenty seven.

01:54

This is our data.

01:56

What do we need to the first Emily's,

01:57

since it is yours, maybe you can help us.

01:59

So what I did was I

02:04

first calculate the marginal frequencies.

02:08

So that's given under the totals.

02:12

And then I calculate the expected value

02:14

for each each column.

02:18

And then I did that by using

02:23

the formula for expected which I showed.

02:26

So it's just basically taking the sum of the i'th column,

02:30

multiplying by the kth column and dividing

02:34

by n. And I specified FMEA,

02:37

i'th column would be the ability to swim,

02:40

and then jth row would be the age.

02:43

So an example would be I would take 145,

02:46

which is the total people who can swim.

02:52

And I would multiply that by 62,

02:54

which is the total bar.

02:57

People die less than 20 years old.

02:58

And then take that and divide it by

03:01

the total, which is 259.

03:04

And that's how I got 34.71.

03:08

And then you can repeat that

03:12

for are the columns and rows.

03:14

And that's how you get those expected values,

03:16

which are in parentheses on the table.

03:20

So we found the expected values.

03:26

Next step is.

03:29

The stated that hypothesis.

03:30

And if we want,

03:33

we can stay to alternative hypothesis right away.

03:35

But this is certainly something

03:38

that we're thinking about.

03:39

That our null hypothesis is there is no relationship,

03:40

and then our alternative hypothesis

03:44

is there is a relationship.

03:46

So you can state it right away,

03:48

right here when you start or well,

03:50

in this case we stated it a little later.

03:53

Okay? Alright, now, next step.

03:55

Next step, we calculate the chi-square,

04:02

which was given and the formula on the slides.

04:07

So inclusively, you're taking each observed value,

04:11

so the original data but you

04:16

had and you're subtracting

04:19

by the expected value which we found,

04:21

which is given in parentheses and the table.

04:23

And you're dividing that by the expected and summing up.

04:26

So an example would be you take 42,

04:31

which is the number of people that

04:36

can swim and less than 20 years old.

04:38

And you subtract that by the expected, which is 34.71,

04:41

and then take the square of

04:46

that difference and divide it by

04:48

34.71 and do the same for

04:50

every You little section in the world,

04:53

for everything added up.

04:58

And you would get about 7.44.

05:02

So in order to figure out whether or

05:07

not you would reject or not reject the null,

05:10

you would have to use

05:14

the chi-square table given in the slides.

05:15

So your chi-squares statistic is 7.44.

05:20

And you want to find which value will you

05:25

reject the reject or fail to reject the null.

05:29

And so if you look at a table that is under specified,

05:34

I guess like alpha level.

05:39

So I just use 0.05. and if you look at

05:41

table when alpha is 0.05 and degrees of freedom,

05:45

which I just, just, just take enabling.

05:49

This is something that you decide for yourself.

05:52

Okay.

05:55

I wasn't sure, so I just did that,

05:57

but then I also explained it and my conclusion by yeah,

06:01

anything above that you would have

06:04

had the same conclusion regardless.

06:06

Okay, so degrees of freedom can be

06:15

found by taking the number of total,

06:18

number, number of rows you have minus

06:22

one times the total number of columns minus one.

06:24

In this case, there'll be three.

06:27

So if you look in your table with

06:29

degrees of freedom, it goes three.

06:32

You'll see a bunch of different values for

06:34

different levels of our giving us tick.

06:37

And I will open that table.

06:41

Okay.

07:19

So if you look, you

07:22

can just folks into degree of freedom is

07:26

D. You can just focus on

07:28

the row in which degrees of freedom because three.

07:30

And then you want to see where

07:34

your test statistic falls in

07:37

terms of all those different values.

07:39

So you can see that 7.44 is less than 7.815,

07:41

which is alpha equals 0.05.

07:49

So what that really means is

07:52

in order to fail to reject your null,

07:55

you want your test statistic, which is 7.44,

07:59

to be greater than

08:03

a certain chi-squared value you can find in this row.

08:04

So for example, I set it equal 0.05.

08:10

And since 7.44 is not greater than 7.815,

08:14

you fail to reject the null.

08:21

And since I concluded that I will fail to reject it,

08:24

this means that I can

08:29

conclude at a significance level of 0.05,

08:31

that age and ability are independent and therefore

08:35

you can't use age

08:40

to determine a person's ability to swim.

08:42

Thank you, Emily. This was a great explanation.

08:47

Is great.

08:52

Officially, Emily's going to be

08:55

our specialist on chi-square analysis

08:57

if you have any questions.

09:00

Alright, thank you very much.

09:07

It was really good.

09:10

That explanation, that solution

09:12

of a question for the alpha value,

09:15

isn't it supposed to be 0.1 because it doesn't and

09:19

the p-value is greater than 7.8.

09:23

So we should assume the 0.1 instead of the

09:27

0.05. we should not

09:31

assume we should set this value for ourselves.

09:35

If you're comfortable with this value of 0.01. you can,

09:39

in which case, you made just a second.

09:44

You want the table again back right?

09:49

Right here, right?

09:52

So you're talking about this value, right?

09:55

Right. So you can,

09:58

it's not a common value to take.

10:00

For our significance value.

10:03

The common values would be 0.050.01, 0.0.0 one.

10:08

This, this, and this, the common values.

10:15

But if you said that you are comfortable with this value,

10:19

then you will look at 6.251 and you will say, Okay.

10:25

Our a value is, which was 7.44,

10:30

is greater than this one,

10:34

then we can read,

10:36

we can reject our null hypothesis.

10:39

But in this case,

10:43

the significance value is 10%.

10:45

So there is a 10% probability

10:49

that byte rejecting our hypothesis,

10:52

we're making an error.

10:56

But actually said was there is a 5% significance level,

10:58

5%, which is the most common value.

11:06

But each researcher decides for him,

11:09

for herself what to take in in a particular situation.

11:12

So if you did that type of

11:17

analysis, that's correct as well.

11:18

Because those incorrect because doesn't,

11:21

in order to take 0.05.

11:24

doesn't your p value have to be greater than 7.8.5?

11:26

You're answering this question from a different point.

11:31

You're saying you're asking What is my P-value

11:36

and you're saying might be valued between

11:39

this and that, right?

11:41

So you're trying to find

11:43

the probability with which you can.

11:45

Instead, what Emily did was shifted.

11:49

If I take my significance level at 5%,

11:54

then I, I fail to reject.

11:57

You can do it both ways.

12:01

Remember, this lab is

12:04

open lab and you can show everything, you know.

12:06

So it is a different point

12:09

what you are doing, you're saying, alright,

12:11

if I take my significance level at this,

12:13

I can accept it or I fail to reject.

12:17

I can reject it. And instead MO says

12:20

I fail to reject it because she took 5%.

12:24

So he doesn't need.

12:28

The higher you take the significance level,

12:29

the higher is the probability that you

12:33

allow for yourself to be wrong.

12:36

Here. So it doesn't need to be

12:40

like it doesn't need to be greater than the 7.8.

12:43

So it depends on the significance level.

12:50

It depends if somebody gives you the significance level,

12:56

then you can, you can just look 35.

13:00

And if you value is greater,

13:05

then you can reject your null hypothesis.

13:08

If your value is less, you cannot.

13:11

Well, we're saying we failed to

13:15

reject the null hypothesis.

13:17

If someone gives you this value,

13:20

if you want to calculate p value,

13:22

it's just a little bit more

13:25

difficult to calculate the value itself.

13:27

Either you need to use the calculator to calculate it.

13:29

I didn't want you to go through this.

13:32

So for that reason are just

13:34

gave you the table and understand.

13:36

Because if you search, he rejected

13:37

or she didn't reject the null hypothesis.

13:39

She didn't reject the no customers report, right?

13:42

She did not reject the null hypothesis

13:45

because the value is less than 7.

13:47

Right?

13:51

Now, if you're saying,

13:52

I'm comfortable with 0.01.

13:53

You can't reject because

13:56

the value in this case the big weight.

13:57

Ok?

13:59

Usually we do take this value,

14:00

this value, this value, one of this three,

14:04

these are the most common depending on the problem,

14:06

but who knows what kind of

14:09

problem we are going to look at in the future.

14:12

So this table tells

14:15

us for different cases what we can and cannot do.

14:16

Right here. Of course,

14:22

what we wanted to do is to understand how it works.

14:24

So no matter what significance level you took, it's okay.

14:28

Alright.

14:33

Okay. So thinking about a time,

14:35

that was a great response.

14:39

And let's move on.

14:42

Mathematical models.

14:45

Can you show Emily's table again for the quiz?

14:51

Let me do it later, ok,

14:54

because I closed it already.

14:55

I can show that table at

14:58

the interclass. Sarah interests closer.

15:01

So we

15:13

is it big enough? That's fine, right?

15:30

Practically can not see it.

15:36

But I need to

15:38

have all the other things

15:44

that might be useful for you're able to see it as well.

15:46

I can guess. Afterall I put this slide together myself,

15:51

so I still remember what's on it.

15:56

It's a pop quiz for you.

15:58

Okay? Last time we started talking about

16:01

the confusion matrix and

16:06

what it means when you are going to use it.

16:09

And the reason for it to be called confusion accelerates,

16:12

knotted was just a joke.

16:17

So I told you that

16:18

sometimes what's here looks a little bit confused.

16:21

Why? First of all,

16:25

we have some knows that double,

16:27

We have some uses that double.

16:30

What's the actual node and what is this predicted note?

16:32

Why do we have two interests for the actual note?

16:35

And the same for prediction.

16:39

Fro predicted no and yes.

16:41

This confusion matrix allows us

16:45

to analyze the performance of a model,

16:48

which is very important.

16:54

It is to say,

16:56

I am going to use this, something.

16:59

We will talk about it later for my model.

17:03

And let's see if I can train them to train

17:06

my model to make some predictions.

17:08

Later on. Training the model find

17:11

in some coefficients, finding some weights.

17:14

That will tell me

17:16

if I get the data that I do not know the answers to,

17:20

for example, to the classification question two,

17:26

then I can say, OK,

17:30

that point belongs to this category or this class.

17:32

This point belongs to that class.

17:35

And then I can compute how many of

17:38

those I predicted correctly and

17:44

how many I predicted incorrectly.

17:46

If this is my data that I trained the model on,

17:50

the data that I have the answers to this questions too.

17:55

If I have actual information about them.

18:01

Because if I have the data that I've never seen before,

18:06

then my prediction is all,

18:10

I'm going to help.

18:13

I obviously I'm not going to have

18:14

these questions answered whether I'm right or wrong.

18:16

So that's why it is important to know on my prediction.

18:21

What is the percent that I will be answering wrong?

18:27

Correct?

18:33

What is going to be the true

18:34

positive to negative, and so on and so forth.

18:36

And what is the value of o?

18:38

It seems like if I look at the accuracy,

18:42

that's all I may want.

18:47

Accuracy shows how many times my prediction is correct.

18:49

So I had two positives, two true negatives.

18:55

And I divide by the total number of

18:58

response or the total number of cases.

19:00

In this case, 0.91 is my accuracy.

19:05

So in 91% of cases I am write my model is correct.

19:11

Of course, I can change

19:18

the efficient cell weights, perimeters in.

19:21

I can get a little bit better accuracy,

19:26

a little bit of an accuracy,

19:29

but I'm quite happy with measuring accuracy by itself.

19:31

Let's see. What does it

19:39

mean that I am measuring accuracy?

19:43

That means I am saying when I M right,

19:46

so I concentrate on

19:49

this values to be as large as possible. That's great.

19:52

That's good.

19:57

So the sum of these values right here divided by total.

19:58

Now, what, what happens if I am a little bit wrong?

20:03

If I give some false positives or false negatives,

20:12

what's going to happen?

20:19

And why should they

20:22

care about it if my accuracy is good enough,

20:25

looks like what do you think?

20:28

Maybe we're very good

20:35

at predicting when something will be true,

20:38

but maybe we're not so good at protecting,

20:40

predicting when we're going to be or in predicting No.

20:42

And then we have OK,

20:45

but still, I'm good.

20:48

I'm in length, at least 90% of cases.

20:49

I'm good.

20:54

That's quite a bit right? Or maybe 95.

20:56

I don't know. I think the idea is just trust any results.

20:59

Just like one result,

21:06

you shouldn't just trust a 100%.

21:08

You should keep in mind that your model is great.

21:09

Thank you so much and that's true.

21:12

But this is what air can receive, tells me.

21:14

The higher this value for the better.

21:18

Or I should pay attention to something else.

21:21

Why do we look at precision and recall?

21:25

Last time we talked about it,

21:30

remember about the precision and recall as well.

21:32

What are the cases when my mistakes can cost me a lot?

21:38

When i wouldn't even care about accuracy.

21:46

But when just a small mistake will cost me a lot.

21:50

I'm going to show you this table.

21:58

Would the accuracy even better?

22:03

But I want you to concentrate on,

22:11

for example, this number.

22:14

So it's a false negative, right?

22:19

It's the false-negative, it's one.

22:23

So the accuracy here is very, very high.

22:32

And yet I may be very

22:37

unhappy about this one being missed.

22:40

Why would you think it may be very critical?

22:48

Give me a case when it is

22:53

critical for a patient

22:55

and a covert free retirement home or something like that.

23:00

And if it easily, if it's a false-negative,

23:03

very easily affect everyone else inside.

23:05

Ok, so in this case,

23:09

false-negative result was given

23:11

to a person who is sick and

23:13

that person is going around

23:15

into contaminate and everything around, right?

23:17

Yeah.

23:20

Some other cases.

23:21

Some other cases.

23:31

When it is important. So when you see a data set,

23:38

you understand this is what is important,

23:41

this is what I concentrate on.

23:43

So another example could be like an earthquake.

23:45

So I can say

23:48

a seismograph predicts no earthquake

23:50

coming 9, 9% of the time.

23:54

But then the onetime it is false negative,

23:56

then everyone is very scared.

23:58

Ok. So these are the cases,

24:02

different types of fraud cases, terrorists.

24:06

These are the cases when you don't really

24:10

care how many good predictions you make.

24:13

You care about this false negatives when you would

24:17

prefer to make more mistakes in other category,

24:21

but not right here.

24:28

And when would you prefer not to make a mistake, right?

24:31

Oh, very good.

24:37

When would you prefer not to make a mistake right here?

24:38

Not to give a false positive and false positive.

24:43

A classic example that is usually given is a Spam,

24:52

Spam prediction. Think about it.

24:57

So, how can you explain that false positive in

25:19

this case is really not

25:26

what you want to see when you wanted to,

25:29

rather than this being identified as negative.

25:32

Even if the classifier fire is mistaken. Why explained?

25:37

In spam detection?

25:43

Just talk. You don't have to put it into.

25:47

That identifies like an internship offer

25:50

us Pfam and puts them in that folder and you miss it.

25:53

Okay, now everybody's going to remember this example.

25:56

That's a very good one.

26:00

So if it identifies an email as a spam and destroys it,

26:02

or better, if it puts it into the spam box,

26:08

then you're going to miss it.

26:13

You would much rather go through

26:16

ten emails that are spam and were not identified as such,

26:19

then you would lose

26:24

this very important one that

26:26

were identified as spam and thrown out.

26:28

Okay? So in this case,

26:32

a good measure is precision.

26:36

In the first case, a good measure is a recall.

26:39

In this case, because this is what I concentrate on.

26:43

And in this case, I concentrated on this.

26:47

So I want to reduce this possible, correct.

26:51

So I concentrate on precision.

26:55

Usually it's a fight between the

26:59

two by reducing one-year increasingly.

27:02

So you need to identify what's more

27:05

important for you in a particular case,

27:08

and then you can work on that particular measure.

27:10

Okay?

27:16

So we talked about this last time.

27:18

We already talked about bias and variance.

27:22

One thing that I want to remind you,

27:25

it's very important to think about bias.

27:29

When you thinking about.

27:33

Presenting a model.

27:36

When you are saying,

27:38

I am going to have a straight line

27:42

or a linear function as my predictor.

27:45

And you look at your data,

27:49

you have no clue what it fits.

27:52

What kind of model needs to be used in this case.

27:56

But triggered when not linear.

28:00

You are going to have an incredibly high bias.

28:03

That means you are trying to use something

28:07

that cannot explain your data that doesn't fit your data.

28:12

Don't try. You cannot

28:18

train your model to make it a good predictor.

28:22

When we're talking about.

28:28

First, we come up with some model and then we train

28:29

it to predict the outcome better.

28:33

We're talking about a model that fits your data.

28:37

If you daddy can not be predicted by a straight line,

28:42

then don't even try it.

28:48

It's not going to work.

28:50

Now, if your data is

28:52

somehow can't be predicted with a straight line,

28:54

maybe as it is right here,

29:00

then what you're doing,

29:02

you're saying I will try to fit it, to fit the data.

29:03

You're trying one line, another line,

29:09

another line, another line, and so on.

29:12

And you see which one is going to minimize the loss,

29:14

which one is going to minimize the error?

29:19

Now, to minimize the error,

29:24

we need to understand how to measure for the air first,

29:27

let's see how we are going to measure the vector.

29:32

The first one.

29:37

What we talked about some of

29:39

this and we will talk more about it.

29:42

Let's talk about loss functions.

29:47

They are going to tell us what is our loss,

29:50

what is our ear,

29:55

what it is that we want to minimize?

29:56

This is a famous data set for

30:01

chemical that you've seen before.

30:03

It shows the tips.

30:06

The data set itself shows the tips that were given to

30:08

a particular waiter when

30:14

this was the amount that the customers paid.

30:17

Here, some extra information.

30:22

So this is the percent i mean,

30:28

this is the tip itself,

30:32

which is shown right here as a percent. Right here.

30:34

It's common to give about 15% of the amount of your bill.

30:40

Let's see that particular way too,

30:49

wanted to see if this is indeed true.

30:51

If he could,

30:54

well suppose that it wasn't real story dinner.

30:56

If he could predict from the amount that people will pay,

30:58

the amount of tip that he's going to get.

31:04

And also from some other features.

31:08

Now, is that a month is a good predictor.

31:10

Actually, what is the percent of a tip?

31:14

He's got to get? So this is the percent up to

31:17

and this is just the proportion of the customers

31:23

that gave this particular tool is,

31:26

you can see it's indeed

31:29

somewhere in the era of May 15% be,

31:31

these are a little bit less,

31:35

but then 20% is high enough.

31:37

So we're not quite clear where this value is.

31:42

Somewhere here maybe, right, between,

31:46

I don't know, 12,

31:49

I guess 12 and somewhere around 20.

31:51

So we're going to try to analyze this.

31:55

That's the situation we're going to try to analyze.

31:59

First, let's look at

32:06

the following loss function that

32:11

will determine the error or loss.

32:16

We call it. Suppose theta is our estimate.

32:21

A new customer or a family.

32:28

They spent so much money.

32:32

And I predict that the tip

32:34

that they're going to give, this is my theta.

32:36

That's my prediction.

32:39

Might estimate points.

32:40

Why I, Y1, Y2 is the data.

32:43

So I do know that output right here.

32:47

This is something that I know.

32:51

What I wanted to see is the distance

32:54

between what I know and what I predict between a,

32:58

what is the mistake?

33:04

Mistake, but I'm going to

33:07

make every time because my prediction,

33:09

the function that we're going to use,

33:12

or at least level start with this one is

33:16

the mean squared error, mean squared error.

33:18

Let's see how we calculate.

33:22

Don't concentrate on this year.

33:24

Suppose we only have one data point, which is 14.

33:27

We have a customer that gave 14%.

33:34

Because suppose our prediction is 11%.

33:40

Let's calculate the loss.

33:46

The loss is calculated as the difference

33:50

between this 14 and our prediction,

33:55

which is 11 squared times one over n. In this case,

34:00

we only have one data point,

34:07

so it's going to be just 14 minus

34:09

11 squared, which is nine.

34:13

So our loss in this case is nine.

34:17

We will say, alright,

34:22

well let's assume it is 12.

34:24

What's the loss in this case?

34:27

14 minus 12 is.

34:30

Two squared is going to give us four.

34:32

Remember, we only have 1 right here,

34:36

so that is going to be one.

34:38

Alright? It's four right here.

34:41

If we have theta 13,

34:44

it's one liter, 14.

34:45

That's exactly what we predicted.

34:47

And it's equal to the data,

34:51

so the loss is 0 and so on.

34:54

The minimal loss right here.

34:56

And if I predicted 14,

34:59

then it would be the best result.

35:02

So this is how we calculate loss

35:05

in how we're looking at the minimal loss.

35:09

If we only have one data point,

35:14

suppose we have five.

35:16

As in this case,

35:19

we do the same for each

35:20

as we did for the one on the previous slide,

35:24

we calculate the distance between each of

35:29

our data entries and our prediction.

35:33

So 11 minus 12 squared is going to be one.

35:39

Right here is going to be one,

35:44

then 12 minus 12 squared,

35:47

then 15 minus 15 squared.

35:50

I'm being sloppy about the order because I square it.

35:53

Anyhow, that's the reason we don't want this signs.

35:58

That's why we never take it linear.

36:03

Because we don't want to,

36:06

for the data, positive and negative.

36:08

If we make a lot of mistakes,

36:12

they will eat up themselves

36:15

if they're both positive and negative bright.

36:16

So we don't want to say that.

36:20

We always use the absolute value or square.

36:22

So the sign doesn't matter right here.

36:27

We just calculate the distance between one and the other.

36:31

Will get this case square root. Is that clear?

36:35

That's an easy formula, right?

36:41

So we calculate the loss for each theta.

36:43

We don't know what theta is. We're seeing.

36:47

What if it is 12, whatever it is,

36:49

13, what about 14?

36:51

And so on and so forth. Later on,

36:53

when I see all this values,

36:55

I will say, OK,

36:58

it looks like this is the best one.

36:59

So I can go with 15.

37:01

Now.

37:03

Not really.

37:05

But close.

37:07

In a couple of minutes.

37:10

I will show you why not really, but we can do that.

37:12

Alright. We look at our original data set.

37:18

The previous one was just an example.

37:26

Then this is the picture that we are going to get.

37:28

If we predict theta being 14.5,

37:31

then that's going to get our loss.

37:36

Here. They are, they are just using this same formula.

37:38

So it looks like,

37:42

seems like something's happening right here.

37:45

Need to analyze more.

37:49

We don't know exactly where it's at.

37:51

It's not exactly 15 or 15.5 or here.

37:53

I don't know where it exactly.

37:58

Right.

38:01

But I can see.

38:02

That is probably in this area.

38:04

If I know that my function has one minimum, one minimum.

38:07

Now, this is a square function.

38:17

So it's going to be a parabola.

38:19

X square function is a parabola.

38:23

Well, it could be parabola with branches down.

38:25

But in our case,

38:32

so we're going to have one minimum.

38:34

The question is just to find that minimum.

38:35

These are our values,

38:39

values for theta and loss.

38:43

So this is the curve that we're looking for.

38:47

This the curve. What we need is the minimum.

38:51

How to find that

38:56

minimum will tell us someone who remembers calculus.

38:59

Hoops. Are you there?

39:09

We'll find the minimum when d L d theta is 0.

39:12

What is d L d theta?

39:16

Ok, so you do remember that's right.

39:27

So you find the derivative or the gradient.

39:31

And you find the derivative.

39:35

And you will find, yes,

39:46

when this derivative is equal to 0,

39:49

this will be a point of your extreme weather.

39:52

It is the minimum or the maximum.

39:55

You can check by looking at

39:57

the value of any point here and point.

40:03

Correct.

40:13

So your function, if your function

40:14

changes the behavior from decreasing to increasing,

40:17

or the derivative is negative to positive.

40:24

Then you're dealing with the minimal and vice versa.

40:28

You are dealing with the maximum.

40:32

This is just a quick review.

40:35

I will not ask you to do that class.

40:37

I'm trying to avoid

40:41

using calculus because I don't

40:45

know how many of you have this classes.

40:47

Hopefully it did, but I don't know,

40:50

but that's the idea.

40:52

I do advise you to review this

40:53

just to understand what exactly we are doing.

40:57

And why would I want you to

41:01

think right now about is both

41:08

a slide that I want. I want something else.

41:11

We will go later through more examples.

41:16

But what I wanted to think right now,

41:19

if your function only has one minimum,

41:22

that you're going to reach it.

41:26

If your function has several,

41:29

and then you have minimum, maximum and so on.

41:33

You gotta be very careful here.

41:36

We're going to talk more about it next time. I hope.

41:38

What we did today,

41:43

you understood it was quite easy hopefully,

41:45

but I don't want to run.

41:49

What I would like you to

41:52

do this time on your own later on.

41:54

I'm not going to go over it in class.

41:58

I want you to look at

42:00

the following function, mean absolute error.

42:02

The idea is absolutely the same.

42:07

It's on slides.

42:10

We're not going to use mean absolute error.

42:11

Well, or if you want for some tests,

42:14

if you make a decision to use it, that's fine too.

42:16

But we will talk why sometimes we

42:20

prefer mean absolute error. Just look at it.

42:23

Again, the idea is the same,

42:26

but instead of taken a square,

42:28

in this case, you're taken

42:33

just the absolute value so that the rest is the same.

42:35