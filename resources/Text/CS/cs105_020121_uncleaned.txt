00:00

machine learning part to what

00:05

you've been waiting for for quite some time.

00:07

I can tell you that you can lock to

00:09

work in data science without

00:13

knowing statistics and probability.

00:15

But what I think is

00:17

most interesting is this part of machine learning,

00:22

data data mining and

00:26

this kind of data analysis to

00:32

where you're not just

00:36

learning what's happened to your data,

00:37

but when you can predict what is going to happen,

00:39

when you look at

00:45

your data and when you can create a pattern,

00:48

when you can understand the pattern behind the data,

00:51

when you can no more than another person who is simply

00:54

looking at this data and doesn't know

00:58

this methods can see.

01:00

Today we will start with mathematical models.

01:04

A model is some representation of your process.

01:08

Usually at the beginning,

01:13

you don't know how you process can be represented.

01:15

The models can be very, very different.

01:20

And without having a little bit of practice of knowledge,

01:22

and you won't even be able to start,

01:27

or you won't even know what to start with,

01:29

how to model this. Who knows?

01:32

I have no clue.

01:34

But the simplest model that we're going to start with,

01:36

we're going to model the process with a straight line.

01:39

You can model with a straight line.

01:44

You can model with a polynomial

01:47

of any degree with a cube,

01:48

with a shape of a human,

01:50

with a color, with just about anything.

01:52

If you know a little bit about your data,

01:57

a little bit about your process,

01:59

you can start with some model,

02:01

and if that doesn't work,

02:03

then you will understand and you can switch.

02:05

This is the phrase that I liked,

02:10

so I put it right here.

02:12

When you have two competing theories that make

02:14

exactly the same predictions with

02:17

the simpler one, the better.

02:20

Do not try to over-complicate.

02:23

Start with something that is simple.

02:26

What types, types of model for

02:31

revenue mathematical model in this case, there could be.

02:35

It doesn't mean that we are going

02:39

to look at all of these models.

02:40

We're going to look at the simplest.

02:42

But then if you're interested,

02:44

you can always learn more.

02:46

But the first, of course,

02:49

to think about linear versus non-linear,

02:51

linear models are the simplest.

02:54

And like I said, we're going to start with linear models.

02:56

Black-box versus descriptive models.

03:00

Well, we can talk about it a lot,

03:04

but to write for now,

03:06

just understand blackbox is something that you put you

03:09

directly and you don't necessarily

03:13

know what's happening inside of it.

03:15

You cannot follow every step of it.

03:18

Well, you can look

03:25

at this first principle versus data driven models.

03:28

You will not understand immediately what it means.

03:32

The first one is something that

03:37

is based on Theorems, formulas, theory.

03:41

Something that you know really well.

03:46

The example right here says,

03:49

for example, Newton's Law.

03:51

You take the formula,

03:52

you take the theorem,

03:54

you know how to plug you apply to you.

03:55

If you understand how to apply it in,

03:58

you get the output,

04:00

the answer to your question.

04:02

Data driven models are absolutely different.

04:04

You are observing your data and

04:08

you're drawing some conclusions

04:12

on the basis of your observation.

04:14

And so on.

04:19

You can always bring

04:21

different types and talking

04:23

about different types of model.

04:24

The more you work, the more you know of the smalls.

04:25

We are going to work on data driven

04:30

models right now before,

04:33

we did look at some of first principles models.

04:36

Now data driven models.

04:42

But there are two types right here that we're going to

04:45

distinguish supervised and unsupervised learning.

04:49

Sometimes you will see the third type,

04:55

which is reinforcement learning,

04:58

which is, well, we'll just describe here,

04:59

but we're not going to go over it later on.

05:02

The supervised models first.

05:06

Understand what does it mean to train a model?

05:11

When you say this data as a straight line?

05:16

So if I know something about this data,

05:23

if I know something that happened with it before.

05:26

And I have this different data points.

05:30

For example, for simplicity,

05:34

audit on to decline.

05:36

I will say, well,

05:37

it looks like I can represent it

05:39

with this line or with some line.

05:42

With some straight line.

05:45

Training. A model is

05:48

finding the parameters or weights for this model.

05:51

If I say that,

05:57

I will try to use a straight line,

06:00

you will ask me to write

06:03

an equation of this straight line.

06:05

Can you give me an equation of straight line?

06:07

In general?

06:10

What you don't have to type it there,

06:17

just seeing MX plus B.

06:19

Yeah. It mx plus b, a x plus B.

06:24

Whatever you want to call them.

06:27

Great.

06:28

But this is just my model.

06:29

I don't know a,

06:32

and I don't know.

06:34

If I analyze my data.

06:37

If I look at the points x, y,

06:41

then I can learn from this points a and B should be,

06:44

they're not going to be exempt.

06:49

They're not going to be determined

06:51

as you would do in your fifth grade math class,

06:54

where you simply computed with one, with one step.

06:59

Just by solving some equations.

07:04

You're going to learn in,

07:07

you're going to save, right?

07:10

I looked and it seems like

07:12

this line represents my data with the smallest error.

07:15

So this is what the training process is going to lead to.

07:23

You will learn some parameters for widths of your model.

07:29

Supervised learning, supervised learning.

07:36

You are looking at your data,

07:41

at your prior data.

07:44

You are teaching your model,

07:47

you are training your model.

07:50

You are telling your model if you use this parameters,

07:52

the loss or the error.

07:57

For now, we will talk about a lot,

07:59

but you can think about air for now is going to be such.

08:00

If you change your parameters a little bit,

08:05

was then your error is going to be a little smaller.

08:08

If you choose your parameters this way,

08:11

your error is going to be greater.

08:13

In what you're trying to do here.

08:15

You are trying to minimize the loss.

08:18

You're trying to minimize the error,

08:21

which ever the set of parameters minimizes the error

08:23

is going to be the set of

08:28

the parameters you're going to use for your model.

08:30

Now, is there a question?

08:34

In unsupervised learning?

08:43

In unsupervised learning,

08:47

the model, can, the data cannot tell you?

08:50

Yes. If you use this parameters,

08:54

the error is such.

08:57

If you use this, the error is such.

08:59

You don't have the answers to

09:00

your questions in unsupervised learning.

09:03

So you are learning some patterns from the data,

09:06

but not necessarily, you know,

09:11

what is correct and what

09:14

is incorrect, unsupervised learning.

09:16

So the supervision is lost. In this case.

09:20

In reinforcement learning, we

09:25

don't necessarily have this.

09:28

Yes, this is correct.

09:31

If I have this input,

09:33

the output is that we don't necessarily have this.

09:34

But there is something or

09:37

some conditions to where the machine is told.

09:40

Alright, if you get this,

09:44

you're getting a reward.

09:46

So we do have some sort of a feedback in this case.

09:47

But this one is

09:52

between somewhat supervised and unsupervised.

09:54

So we're able to say, yeah, that's.

09:58

Good.

10:01

It's called every word signal.

10:02

In this case.

10:03

We're going to start with supervised learning.

10:05

And we will look at

10:09

some attributes of supervised learning.

10:11

What we need to know before we start

10:14

actually on supervised learning.

10:16

Only a training set is available.

10:22

Like we said before.

10:27

In a supervised learning where going to have

10:28

a training set and test.

10:31

A training set is a set that has all questions answered.

10:37

If x gives this white is that.

10:45

And I don't know yet what my test and set is going to be.

10:49

It may come later.

10:54

So I don't have anything to test my data on right now.

10:55

What do I do in this case?

11:01

We take this training set and divide it.

11:04

We use a part of it to train

11:09

the model and another part to test it.

11:12

We pretend that for this part,

11:16

we don't have answers.

11:19

We will say according to

11:21

this training set, my straight line.

11:23

For example, I said we will start with the straight line.

11:28

My straight line is going to be such.

11:31

And then I will say, I use it on the test set.

11:34

I am going to have such error.

11:40

And this error is going to be how far

11:45

these points from the test and set or from my prediction.

11:49

It's okay if you don't quite

11:56

understand because we will go through this example.

11:59

I am just telling you what is testing,

12:01

what is a training set and what is a test and set?

12:04

So we will pretend that

12:08

we have a training set and a testing set.

12:11

This simple right here.

12:15

In this case, we're simply partition in

12:18

the set into two training intestine.

12:20

And then we don't know if

12:25

this part is any different from the test and set.

12:27

So what we usually do,

12:31

we use a K-fold validation.

12:33

For example, two-thirds is

12:36

going to be used for training and 1 third for testing.

12:39

And then we will take the first third for testing,

12:42

the rest for training.

12:46

And then we use the second third

12:47

for testing in the respiratory rabbit, correct?

12:50

It would be correct to say that we use this for

12:54

training and the rest for testing process wise.

12:58

No.

13:02

It's also common to use the live one out.

13:03

We use only one data point for testing.

13:08

So we use just about everything of

13:13

the training set of the set that we have

13:16

for training and will leave only one.

13:19

And we're going to test

13:23

the performance on our model only on 1.

13:26

For example, you have this data that you

13:31

collected from the survey

13:35

and you're asking different questions, What you want,

13:37

you want to see if GPE,

13:39

GP depends on some other attributes of the test.

13:44

So in that case,

13:48

you can run your statistical tests

13:50

and you can see whether it does or doesn't.

13:53

Now, in this case,

13:56

when we come to

14:00

some machine learning techniques to

14:01

prediction what we have,

14:03

we have data, for example, for CS101 students.

14:05

We know all the attributes and GPS for all the students.

14:10

Then a new student comes.

14:15

We don't know a GP of the student.

14:17

Maybe the student didn't take an acos As a few Serra yet.

14:20

But we have some data about that student.

14:23

And using the model that you received from training.

14:27

You have said on the data that you have,

14:32

you are able to use the attributes or what you know about

14:35

the student to predict the GPA of that student.

14:39

Okay, so this is what we are going to face.

14:46

We have some data that is available.

14:52

And when a new point comes, point points,

14:55

the whole set, we will be able to make predictions.

14:59

Now, we don't know,

15:07

we have never seen those points.

15:09

We don't know whether those predictions

15:11

are good enough. Mythical.

15:12

Now, usually how it's done is you are those before?

15:17

We're just the techniques for splitting.

15:25

But what do you want to do?

15:29

Normally? You want to divide your fit ahead of

15:32

time into the set that is not going

15:38

to take part in your testing at all.

15:42

We will see we will explain

15:47

the mistake and why we want to do that.

15:49

And you will have

15:52

this part that you will use for training of your model.

15:55

Some part.

16:03

How it's done. You will move this part to the side.

16:05

Don't touch it with the remaining part.

16:10

But this validation is what

16:17

you're going to test your model on.

16:21

This is the whole data.

16:26

Leave some, put it aside.

16:29

Don't touch.

16:33

Train your model on some subset,

16:35

some subset of this training set.

16:41

And in this, in this set,

16:44

you're going to use a part for the validation.

16:47

It could believe one out,

16:51

it could be k-fold.

16:53

In this case it is 5-fold the validation.

16:54

Sometimes it's called cross-validation because

16:58

I'm using the same data

17:02

for validation and for the training.

17:03

So in this case it is a fivefold.

17:08

Normally it's just called K-fold validation.

17:11

If you use the leave one out,

17:14

then if you have a set of n elements,

17:17

it's the same as n fold validation.

17:20

The idea is this thing.

17:23

The idea is the same right here.

17:25

Now, can you guess why would you leave this part outside?

17:28

And you will not allow your model to even see this test.

17:37

To see this portion

17:42

is identify anomalies and kids start one chunk effect.

17:46

So datacenter way that there's not a good try.

17:51

You made some point in what do you

18:10

mean to not include the tests in your dataset?

18:14

Will that change how it reacts to that tests later on?

18:17

Is that a question? Yes. Yes.

18:24

Because part of it is already

18:28

an answer and a good answer.

18:30

That's the point.

18:35

So if I included this test part,

18:37

that means my model is

18:42

going to see all data ahead of time.

18:45

And it's going to be looking

18:50

at all possible variations in my given set.

18:55

It's going to learn everything that is there.

19:01

But whatever comes from outside later,

19:04

after this process is completed may be

19:08

somewhat different from what

19:13

we learn from the given data.

19:16

So by cutting this piece

19:18

and not introduced it into the model,

19:22

I don't allow my model to see everything.

19:25

And I am trying to see how my model is

19:30

going to perform on the data

19:36

that it had never seen before.

19:38

If it's going to perform really poorly

19:42

compared to how it performed on

19:48

the data that it had seen before.

19:50

That means that I overtrained the data,

19:53

over trained the model.

19:57

We will look in a minute.

19:59

What exactly it means?

20:01

It means I learned too much from every little bit.

20:03

It means that there were anomalies

20:07

right here that I thought were normal.

20:09

This is what Richard just mentioned.

20:12

There were anomalies that I

20:13

thought maybe I should account for those.

20:16

Maybe those anomalies were

20:20

somewhat normal, immediate those switches,

20:23

noise outliers that were

20:26

presented because of some sort of pairs.

20:29

Let's say. Before we go to that,

20:34

I would like you to look at the confusion matrix

20:41

because this is what sometimes

20:47

help us to understand how the model performs.

20:51

Somehow we need to measure the performance.

20:55

We cannot say more or less good,

20:57

but let me see if I can do better.

21:02

What is good? What is better?

21:04

We need to know.

21:07

So let's look at the confusion matrix.

21:09

I think it's called confusion

21:13

matrix because it is quite confusing, to be honest.

21:16

So I know that I showed this matrix last quarter.

21:19

And then one question

21:25

on the exam was such that there are,

21:27

these rows and columns were the other way around.

21:30

And a lot of students didn't realize that.

21:33

When you're looking at this matrix,

21:37

do not try to memorize what is where.

21:39

Try to understand what's inside.

21:45

It measures the performance of my model.

21:48

If I know the actual answers to my questions,

21:52

for example, where it working on prediction.

21:58

And we simply have two answers, yes or no.

22:01

It is some classifier that tells me, for example,

22:06

who were looking at these problems before,

22:11

whether that person has this illness or sickness or not?

22:14

Yes or no to,

22:18

in this, in this case,

22:21

we're not talking about this,

22:22

that linear linear model,

22:24

but we're just talking about the classic far

22:28

that tells me glass one plus two.

22:30

Let's say one is the is the other one is known.

22:32

So that actual no,

22:35

we had 5060 cases and the actual Yes,

22:39

we had a 105 cases.

22:44

When we predicted using our model.

22:47

We got no 55 times.

22:53

This was our prediction.

22:58

So that's the correct result.

23:01

And this is our prediction.

23:04

But the predicted no was 5550.

23:09

Bills corresponded to the actual note.

23:16

So that was correctly predicted.

23:19

This.

23:27

Was the mistake. So this is true.

23:29

And this is false.

23:33

I predicted null.

23:36

So this is true negative.

23:39

And this is false-negative.

23:40

I predicted Yes, it,

23:43

in ten cases, it was wrong.

23:47

I predicted yes when it should have been known.

23:49

And we predicted a 100 correctly.

23:54

We predicted a yes when it was supposed to be.

23:58

So true positives.

24:00

100.

24:02

I answered this question correctly,

24:04

my model answered it correctly.

24:06

False.

24:08

We predicted Yes when actual results would have been no.

24:09

Right?

24:18

Okay.

24:20

Next, that's actually describes

24:22

what we just talked about.

24:27

And what you need to remember is this value is recall.

24:29

And this value is precision.

24:39

Precision it predicts how often it is, correct.

24:44

True positives.

24:54

Out of all those predicted?

24:57

Yes.

25:03

True positives.

25:04

Out of all of those that predicted, yes.

25:08

Okay.

25:13

Alright.

25:16

I don't want to run through all of those.

25:18

I want you to slowly look at

25:22

them later on, after the class.

25:25

Ok.

25:28

Slowly, slowly Look at this.

25:29

We are not going to work with ROC curves,

25:35

but they also measure

25:42

and visualize the performance of binary classifiers.

25:45

We're not going to work,

25:49

but I decided to put them right here.

25:50

So, you know how they

25:52

look at lists later when you see them,

25:54

you know what they are

25:56

about the area under the curve that

25:58

also shows the performance of your algorithm.

26:01

Multi-class systems.

26:08

You can see this examples.

26:10

This is just something that's so you know the names.

26:12

We're not going to work with this last three.

26:17

But please understand this matrix very well.

26:20

I don't want to start with any questions.

26:31