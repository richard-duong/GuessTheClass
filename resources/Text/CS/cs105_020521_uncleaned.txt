00:17

We started talking about loss functions.

00:23

And you understand more or

00:26

less what the loss function means.

00:28

In order to train our model,

00:32

we need to understand for

00:35

each particular set of parameters

00:38

how good we are at the values that we want to predict.

00:40

By first running our examples on the training data.

00:47

We are running it on the data that can insert

00:57

our question whether our prediction is correct or not,

01:01

or int for how close it is to the correct value.

01:06

We collect all the information about

01:13

every time when we miss the correct answer,

01:18

or every time when we compute a value,

01:24

we compare it with

01:27

the correct value and we will see how far we are from it.

01:29

It can be done using mean squared error loss function.

01:34

We looked at this function,

01:39

lost time, which way we're doing it.

01:42

We find the difference

01:44

between our predicted value in the correct value.

01:47

Then we find the square of it and all the points.

01:51

And divide by the number of points.

01:59

Add information about each point

02:02

and divide by the number of points.

02:04

This is going to be our loss.

02:07

Once we compute the value of

02:11

this expression is going to be our loss.

02:13

We can take this in this example.

02:16

We started with theta equals 121314.

02:18

And so one, moving this wave,

02:21

we'll find the point with the smallest loss.

02:24

We also said last time that

02:30

this function is a quadratic polynomial.

02:35

So what we technically what to do.

02:40

We wanted to do the minimum of this function.

02:43

We want to find the minimum of this function.

02:46

How can it be done?

02:48

It can be done by finding credit.

02:50

So what we'll find the derivative and we

02:57

find where this derivative is equal to x2.

03:01

By checking the behavior of it.

03:07

So this value, or let's

03:16

say to the left of

03:19

the valve and to the right of the value.

03:21

We will confirm that this is indeed a minimum.

03:23

Maximum, right? We don't know what this fun,

03:27

not this particular function,

03:31

just in general, what function behaves like.

03:33

So that's how we confirm that the extremum isn't minimum.

03:36

Now, are we going to do it actually,

03:43

quiescent technical person does everything for us.

03:48

But you need to know what this

03:51

function's perform and how

03:55

they perform what they do exactly.

03:58

For that reason, I included some formulas right here.

04:01

So if you have enough of a background in calculus,

04:06

you can go through those.

04:10

If not, at least you understand the idea of the process.

04:12

Even if you don't look at the formulas,

04:16

you understand the idea of this process.

04:18

What is shown right here,

04:23

that for the value that we're looking for, that value,

04:25

that means squared error is going to give us,

04:30

is the mean, is the new mean of y mean Apollo.

04:34

And I told you that we want to know

04:45

about this mean absolute value

04:49

that behaves, not behaves, that.

04:51

The idea is the same.

04:54

We look at each given data point

04:57

and find the distance between

05:00

our data points and our currently predicted value.

05:02

And we compute the loss function that

05:08

is going to be the sum of all of our,

05:11

nonetheless, the sum of all of

05:14

our errors were only taken an absolute value.

05:17

All it of course, we cannot afford

05:21

not taken absolute values

05:25

because if we have values with different signs,

05:27

then the GUI will not go into,

05:31

have an information about the error.

05:39

Because negative values, when you add

05:42

two positive values will reduce the number

05:46

of fear and that's what we want to avoid.

05:48

And of course we divide

05:50

by in compute the sum seven divide by n.

05:51

You cannot compare the values

05:56

themselves of this function.

05:58

And this function, because that is going to be

06:00

square root of that is going to be an absolute value.

06:02

So obviously the values are

06:05

not comparable sometimes right

06:07

here is use root mean squared error.

06:09

So that is done to what you need

06:15

to know about this squared error in absolute value here.

06:20

When do you want to use and what you

06:26

need to think about when you're using them.

06:29

This is a squared error and this is an absolute error.

06:35

But they are, they are very close,

06:44

not the value wise,

06:49

but behavior was, their behavior was,

06:50

is closed closer to the minimum,

06:53

the minimum of that function to the extremum.

06:58

And they behave very differently for the outliers.

07:01

Well, not necessarily just for the outliers.

07:07

The further you are from the mean and median values,

07:10

which don't have to be the same.

07:15

Obviously. The more is

07:16

the difference in their behavior is you notice

07:20

the squared error function

07:24

penalizes much more for

07:29

those values that are further from the mean.

07:32

And especially for outliers.

07:35

Especially for outliers.

07:38

It gives much bigger penalty

07:40

compared to the linear function.

07:44

Just think about it this way.

07:51

1234. If we are one unit apart,

07:54

then square root error will penalize by one.

08:05

Absolute value will penalize by one.

08:11

If we're to units of port,

08:14

the absolute valuable penalized

08:17

by the squared, penalized by four.

08:19

So you can see that,

08:25

well, you can understand

08:30

that absolute value penalizes linearly.

08:31

And of course, squares penalizes

08:34

this by taking the square,

08:36

by multiplying by itself.

08:41

So if you do not want to penalize for the outliers,

08:44

then you're going to use the absolute value.

08:49

If you want to pay attention to those values that are

08:56

further than he wanted to take

09:01

a squared optimization problems.

09:03

A lot of times they're just using

09:06

a squared error loss function.

09:08

A lot of times you are using the function that is

09:12

a combination of the two. In a second.

09:16

We're going to show them.

09:21

All right.

09:25

So we looked at

09:28

the squared error and showed that it is mean.

09:30

Absolute error is giving you the median.

09:35

You know that the mean and the median

09:41

are not necessarily the same.

09:42

And, you know,

09:44

the cases when you would like to use the mean,

09:46

when you would like to use the medium.

09:49

The Heber loss function that combines the two,

09:54

combines the advantages of each function.

09:59

As you can see, if the distance.

10:03

Between the points.

10:07

If the error is less or equal than some value,

10:09

then you are using the squared error.

10:14

And if the distance is greater,

10:16

so if you don't want to penalize

10:20

as much for the extreme values,

10:22

for the values that are forth from from the millimeter,

10:26

then you can use the absolute,

10:31

the absolute error function.

10:34

But as you can see,

10:37

there are some parameters for right here that

10:39

allow you to compare the values,

10:42

otherwise they cannot be compared.

10:46

How do you choose alpha right here?

10:52

And we're not going to talk about this alpha right here.

10:54

We'll be talking about some gradient descent

10:58

later on, okay?

11:00

Because in this case we're

11:02

not going to look at Heber lost.

11:04

A lot of times in machine learning.

11:09

Pickin parameters is an art,

11:13

an art that is coming from your experience,

11:17

from your knowledge, from your knowledge of data.

11:23

So a lot of times when I say an art,

11:27

it doesn't mean that it is just coming from nowhere.

11:31

You close your eyes and you think that's a pretty number.

11:35

What I mean is there is

11:38

no formula that will tell you alpha is this.

11:40

Okay? We will talk about this perimeters.

11:46

So I don't want to just to rush through it,

11:50

we will talk about the parameters, gradient descent.

11:52

So when we weren't to loss functions,

11:58

we said let's pick theta equal to 12,

12:02

then 13, then 14, and so on.

12:05

How do we know where to start?

12:08

How do we know how big those steps should be?

12:10

12-13, maybe I should have taken 1215,

12:15

maybe I should have taken 12.512, and so on.

12:19

How do I make the decision of the next theta?

12:23

For our tips dataset,

12:30

it is a relatively small set.

12:33

We know that the minimum values,

12:35

that the maximum values then basically we can

12:37

just run from the minimum to the maximum value.

12:39

Now typically it can be from a 0 to a 100.

12:43

If we'll look in on a graph,

12:48

because that was a percentage rate.

12:51

When we're looking at a graph, we can estimate.

12:53

And we did it last time.

12:57

Well, that was somewhere between 10, 20%.

12:59

So we can try beta between 1020.

13:03

But obviously, we don't want to

13:06

depend every time on a particular dataset.

13:10

On us seem a visual information about this dataset

13:16

and for us knowing where exactly this value can be.

13:23

So there were first a lot of

13:31

assumptions made for our tips dataset.

13:33

And second, we did a lot of work before we even pick.

13:36

That theta, that was 12.

13:40

You didn't think about it, right?

13:42

Because it was neutral.

13:44

Here's a dataset.

13:46

Let's look at the graph.

13:47

It looks like this is what we can do.

13:49

But what if we don't have

13:52

that information or the data set is large?

13:54

Or if we wanted to create

13:57

some packages that are not going to be data dependent.

14:01

How can we solve this problem in general?

14:09

So suppose we didn't know that.

14:14

It's good to start at 121012, doesn't matter.

14:19

Let's see. What did we do?

14:23

We created at graduate.

14:28

Remember, we looked, after,

14:33

looking at 1214 and so on.

14:36

We said, let's look at the derivative of this function.

14:38

So we looked at this line,

14:44

at the behavior of this line,

14:49

which is tangent to this particular curve.

14:52

At this point.

14:56

Let's see how this line behaves. All right?

14:59

It was, we were moving

15:06

slower than this line would continue.

15:10

Heaven.

15:14

Here we go.

15:16

Right angle.

15:19

The angle of having an acute angle,

15:24

right here, this acute angle.

15:31

But if we jumped too far,

15:34

then the angle would have changed to the opposite.

15:37

It would have changed to

15:43

the tubes depending on

15:44

which direction we're talking about.

15:47

But this sign would have changed the direction.

15:49

Would it change?

15:52

That means what would it mean by the way?

15:52

What would it mean that this angle changed?

15:59

So we found the derivative,

16:11

that's a function, right?

16:14

We found the value.

16:16

We found that this angle changed to that angle.

16:19

What would it mean?

16:26

You just subtract the gradient.

16:34

What would it mean if the

16:37

accused changed to acute or vice versa?

16:41

That we missed the point,

16:46

that we missed that point and we need to go back.

16:49

Look, the line goes here, here,

16:55

here, here, at this point,

16:58

it's parallel to x axis.

17:01

And then the angle changed.

17:03

So if we go from here to here.

17:06

If we took too big a step,

17:11

then we miss this point.

17:15

If we went from 12 to 17, for example,

17:18

then we missed the point and

17:23

the next value of theta should not go up.

17:24

It should go back. It should go down.

17:27

Is that clear? Because you

17:30

you learned some analysis or calculus, right?

17:36

Did you learn it? Because if

17:41

not, you've got to tell me right now.

17:44

I'm not sure, but if you continue

17:53

to just subtract the gradient,

17:56

then it shouldn't matter.

17:58

It matters which direction we go.

17:59

We can continue moving.

18:02

181920 and is the one we're going to be somewhere else,

18:04

not where we're supposed to be.

18:11

Let's look at the following example.

18:13

How we are going to give an optimization.

18:20

Our goal is to find a local minimum.

18:25

Now, all differentiable function,

18:31

I should be able to find

18:37

a derivative of this function, right?

18:40

The gradient descent is good to find a local minimum.

18:43

If this function has several minimum

18:47

or that are the same or different.

18:50

And I'm somewhere here,

18:54

I'm going to find this minimum.

18:57

I'm going to move right here.

19:00

If I make huge step,

19:03

I will end up being here they are somewhere else.

19:07

But in any case,

19:10

whatever the minimum I find,

19:12

it's just a local minimum.

19:14

I don't know if it's global as well or not.

19:16

Alright?

19:23

That's my function.

19:25

X plus five squared.

19:27

I take a point on the curve at some point that

19:31

belongs to the upper right-hand,

19:37

I find the gradient of the function,

19:42

which is going to be the derivative of this expression.

19:46

Two X, two X plus 52 times X plus five.

19:49

That's just times.

19:55

I can find the value.

20:00

At this point.

20:05

What I want to know is to setup some learning rate.

20:08

Again, it comes, often it comes from the experience.

20:17

But a lot of times I'm just going ahead of time.

20:23

But a lot of times with taken for the learning rate,

20:27

some power of ten.

20:31

So could be 0.1. could be 0.01. could be 0.0.0, one.

20:33

So ten to the negative 12345, something like that.

20:38

Now, if we are afraid of skipping the minimum,

20:42

then we're taken a realist small value.

20:49

If we know anything about our function,

20:54

we may know what is small.

20:57

Give me a second and we'll get back to this question.

21:02

But if we don't know

21:06

anything and if we want just to go through

21:08

the function will then sometimes we

21:11

want to take a little larger of this

21:14

felt you and see

21:17

if there are other minimum in

21:20

this functions because it made,

21:22

bring me to the different minimum from here to there.

21:25

For example, in sometimes we alternate.

21:34

We're not going to go through all of this.

21:37

We're just going to work on the local minimum for now.

21:40

Ok, we're only going to try to find this minimum.

21:47

That's all we have sit in

21:53

up the learning rate, which is 0.01.

21:56

And we will see what is going to happen.

21:59

If I go,

22:04

if I go to

22:13

the left with this rate, what's going to happen?

22:16

Where am I going to beam?

22:20

Why would I go to the left?

22:24

Because the direction of the tangent

22:26

tells me where my local minimum is.

22:29

I am, I am trying to avoid calculus,

22:36

at least some calculus you need to understand.

22:50

Why did he choose 0.01?

22:54

Just for this example.

22:56

A lot of times we may just want to choose 0.01 or 0.01.

22:59

These are just some values that are common to choose.

23:06

Will still say you got that question.

23:13

I can explain right now.

23:15

Of course, the smallest value we choose,

23:17

the less is the probability that we miss this minimum.

23:22

However, if our value

23:29

is too small, then what's going to happen?

23:32

If our value is too small,

23:41

it will take more iterations, right?

23:44

So it will take a very long time.

23:46

I'll be just sitting right here and moving very, very,

23:49

very slowly because the learning rate is

23:53

the speed with which I learned.

23:55

So if I have a very slow speed,

23:59

then I learn slow.

24:03

So sometimes I may

24:06

want to see nothing is happening right here.

24:09

Let me increase the speed.

24:11

Alright, so how it works, how it works.

24:14

We have to perform several iterations.

24:18

Here. At each iteration,

24:21

my goal is, well,

24:25

my ultimate goal is to reach this minimum, right?

24:28

But each iteration is

24:31

going to give me the next point right

24:33

here is going to happen.

24:36

X1, or the next point is

24:40

the point that I'm looking for in iteration one

24:43

is going to be x 0 minus something.

24:47

How big is this something?

24:57

It's going to be learning rate

25:02

times d y by d x evaluated at x 0.

25:06

So our previous point is negative three minus

25:16

our learning rate times

25:22

evaluate dy over dx at minus three.

25:26

And you will need,

25:38

between now and next Wednesday,

25:42

you will need to review

25:45

a little bit of trigonometry, sine,

25:46

cosine and tangent, and a little bit of calculus,

25:51

the derivative and what it means.

25:58

Okay?

26:01

So that should be enough.

26:03

This way, I am going to find the next point.

26:07

X1 is the next point, right?

26:17

This is the next point.

26:21

I am moving to

26:23

this point and I perform the next iteration,

26:24

which is going to look exactly

26:27

like the previous iteration.

26:29

It's just going to start at a different point.

26:31

So we're going to,

26:35

we're going to perform

26:39

the next iteration and find the next value,

26:41

the next value and so on.

26:43

Next, when am I going to fit?

26:48

Media should stay right here.

26:52

When am I going to finish?

26:54

I'm going to finish when the values almost don't change.

26:59

Usually when we say we change,

27:06

we stop when, when theta is the same.

27:11

Well good prediction.

27:17

But it's never going to be the same.

27:20

So you'll sit enough some threshold and your say when

27:23

the change between the previous step and

27:26

the next step is such.

27:28

And you decide for yourself what you want it to be.

27:30

It depends on your function.

27:33

Sometimes, sometimes you're just saying, well,

27:35

okay, 0.01. just to set up some threshold.

27:38

So that's the value.

27:42

When it stabilizes Samuelson, alright, I'm done here.

27:44

I want to stop my album from running.

27:47

Or sometimes you just want to say,

27:51

I perform so many iterations.

27:53

But in that case,

27:57

you'd better know at least approximately the behavior

27:59

of the function. Okay?

28:03

So we start with a lot of times.

28:11

If we don't know anything where to start,

28:16

then we're just saying let theta be 0.

28:18

That's it.

28:21

I have no idea where to start.

28:22

Let it be 0.

28:24

This is some calculus.

28:31

If you wanted to go through this calculus,

28:34

I would think you should.

28:38

If you have enough knowledge.

28:41

If you don't, you will not have

28:45

as deep an understanding of

28:49

the graded descent and the regression.

28:51

However, that's not going to change.

28:54

Your understanding of the process gets so.

28:58

And if you worry at this point that

29:02

is going to bring down your great, then no.

29:05

So I am trying to accommodate I'm trying

29:09

to accommodate as much as

29:15

possible all students in this class.

29:18

If I just remember,

29:21

if I skip through the slides,

29:22

it is some additional information

29:25

not because I don't have time to corporate those,

29:27

just because sometimes I think it may be

29:30

difficult for some students that

29:33

are on their second year,

29:35

but it may be useful for some other students.

29:38

So understand that if you feel like it, you go over it.

29:42

If you don't, then it's okay if you skip it.

29:46

Okay? So what is valued at?

29:50

Great question, what is this value?

29:56

What is this value right?

29:58

Now?

30:01

Like I said before,

30:03

this perimeters that we cannot setup in a scientific way.

30:04

There are some of those that we need to either decide

30:15

on from our knowledge or just say,

30:20

okay, let it be this and see how it behaves.

30:24

Okay?

30:27

So for this learning rate to

30:28

the rests on common values to use,

30:30

like 0.01 is a very common value.

30:33

But if you see that you are not moving very far,

30:37

you may want to increase it.

30:41

If you see that you're jumps are very large,

30:42

then you may decrease it.

30:45

Now, you're not going to worry about it in this class.

30:48

If you get to machine learning class later on,

30:53

that maybe just, maybe you will.

30:56

Sometimes you just want to look at different values.

31:00

You want to run your algorithm by Saint.

31:03

Alright, let me try to run this algorithm with

31:07

alpha being 0.01 or alpha being 0.001.

31:09

Now a common strategy is to

31:15

start with a little larger outfits.

31:20

And then when you see that you

31:26

are approaching your minimum,

31:29

then you're making the values of alpha smaller,

31:31

so you don't miss it.

31:36

So this way, you are not moving too slow,

31:38

but on the other hand, you're not going to miss it.

31:44

So depending on how

31:47

large the steps that you make in right here,

31:50

your change in your learning rate.

31:54

You're not going to deal with it in this class,

31:58

but this is just if you're interested,

32:02

how alpha is big.

32:06

So we may want to start with

32:08

the larger alpha and then make it

32:11

smaller when we approach him.

32:13

Alright?

32:19

And basically this is what we just discussed.

32:26

This is the same as I showed you on that example.

32:31

For the parabola.

32:36

I showed that simple examples so you

32:38

don't get scared right away.

32:40

I've seen this formulas.

32:43

So you can see this is our previous style.

32:46

A bit learning rate and evaluating

32:49

the gradient descent at

32:52

that point, at a particular point.

32:54

So that is going to give us the value

32:59

for the next iteration.

33:02

Alright, more or less.

33:05

And this is just a little bit more.

33:15

But I'm looking at you and I don't know.

33:18

I probably should not date worrying about those.

33:22

As, as I said before,

33:25

you can only find a local minimum

33:29

by using gradient descent if you jump to four,

33:32

right here, for example,

33:36

you are going back and you'll find in this local minimum.

33:37

But if you jump in too far,

33:40

then you may just find this one.

33:43

In any case.

33:45

In any case, it's going to find just the local power.

33:48

Of course, you can use

33:52

some calculus or analysis

33:57

to first see the behavior of your function.

34:00

And then you will know more what to use the gradient for.

34:02

But it's out of scope of this class.

34:07

So what we are going to do is we're going

34:10

to just find this particular minimum.

34:13

If the function has only one minimum,

34:17

then gradient descent is

34:22

perfect for the case because no matter what you do,

34:24

you're going to find your local minimum.

34:27

Well, no matter what you do,

34:30

if you know what you're doing, of course.

34:32

Because if you set up

34:34

a large ALS heart and he don't change it,

34:36

then you can just moving back and forth.

34:39

With itself. Okay, so I'm

34:42

going to stop talking about loss functions right now.

34:47

Our next lecture is going to be on regression.

34:52

We will be talking about regression analysis.

34:56

And this will be

34:59

the first technique that will

35:02

be a candidate for your project.

35:03

Three techniques for your project are

35:07

going to be regression.

35:10

Case near k nearest neighbors.

35:15

K-means clustering and hierarchical clustering.

35:20

So you will need to choose three,

35:24

either out of this four.

35:28

But that we will study in class or out of those that,

35:30

you know, if you took,

35:38

if you took machine learning class,

35:40

maybe you know something else, some other techniques.

35:43

I don't mind if you're using those.

35:45

Believe me, I know all of them so well.

35:48

Not all of those that exist,

35:51

but certainly those that he'll

35:53

learn in machine learning class.

35:54

So I'm fine with it.

35:57

If you want to run something for your project,

35:59

you will need to find some dataset

36:02

is suitable for you running this techniques.

36:06

So do not rush into findings some datasets that

36:11

you just like because it's possible

36:17

that this is an interesting data, interesting dataset.

36:19

Interesting question, but you cannot perform

36:22

this particular techniques on this, on this dataset.

36:25

So we start to look him,

36:33

he, Mr. thinking about it.

36:35

But if you don't,

36:37

it is true. You do not have to.

36:38

I just had some questions about the final project,

36:41

so I wanted to at least

36:44

to get closer to what it's going to be.

36:46

That.

36:49

And also, we're going to look a little bit

36:51

later at the predictors.

36:54

If of course, you're using

36:59

Amazon or something like that to purchase something from,

37:04

or to watch movies or to play games.

37:09

You know, when the attrition one product,

37:12

you see some predictions

37:14

about the other products that you may

37:16

want to purchase by play, test of anything.

37:19

So I will show you how you can

37:26

possibly construct, build such predictors.

37:29

And if you want to have

37:35

your project to debt, that's fine too.

37:36

But it may be a little bit more difficult.

37:38

But if you're interested,

37:43

certainly you can do that and you can

37:44

always have it on your resume.

37:46

That's interesting too.

37:48

And also we're going to perform some text analysis.

37:50

If you want to work on text analysis

37:55

gets fine with me as well.

37:57

So basically, this project is pretty open.

37:59

Again. You can choose what you are interested in.

38:02

But within the material that we study in this class

38:08

or something similar that

38:13

you may have learnt from different class.

38:16

Now, I don't advise you to

38:19

start learning these techniques.

38:21

If you've never learned them before.

38:24

Because it's going to take a lot of time

38:27

and you're still go into the most

38:30

likely shake it on them.

38:32

So I would suggest take something that

38:35

we either doing this class or you learned before.

38:38

This is my suggestion.

38:41

I wouldn't mind you're learning something new,

38:43

which is always great.

38:46

But I understand that when you have

38:47

only several weeks for your project,

38:50

it may not be reasonable.

38:52

Don't set goals that taking her not to reach.

38:54

Okay.

38:59