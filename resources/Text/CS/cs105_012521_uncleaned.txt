00:02

And this is exactly what you're

00:04

going to do after you collect your data.

00:06

Whether whichever type of data collection you are using,

00:14

whichever type of data you are using,

00:20

you will have to deal with

00:23

some preprocessing, more or less.

00:24

That depends on data,

00:27

that depends on the task that you are going to perform.

00:28

In a lot of cases,

00:33

data preprocessing in a very long process.

00:36

Sometimes, sometimes scientists say about 80% of

00:40

time that you spend on

00:45

your projects is taken by data pre-processing.

00:47

So be ready that data is not

00:51

going to come to you the way you want to see it.

00:55

You will have to work on it before you even start

00:58

to running any of the tests that you learn in this class.

01:02

The first step is data cleaning,

01:07

then data integration, integration transformation.

01:11

And you're going to go through data reduction.

01:16

We will talk about each process and Ben,

01:21

you will go through all of these processes

01:25

during the next lab when

01:28

you collect your data from service.

01:30

Believe me, data that you collect is going to

01:33

be worse than any data that is already published.

01:37

Because what is published

01:41

is already a little bit cleaner,

01:43

a little bit brushed more or less.

01:44

But it is in some way it's already taken care of.

01:47

You are going to get

01:52

data as raw as it gets. Good hands dirty.

01:54

First.

02:00

When you see your data, check data quality.

02:01

Just screen, looks through accuracy.

02:07

Sometimes you can't tell whether it is accurate or not.

02:12

For example, a simple question above to the age.

02:18

Whether you needed for the Serbian nod,

02:22

but a simple question.

02:24

And if you see that somebody put negative five or a 130,

02:25

your already questioning this numbers, right?

02:31

So something may be

02:36

inconsistent with the information that you expect

02:39

it to get that you see immediately it is drawn.

02:42

What you need to do at this time, you need to.

02:45

Usually you may want to check the row and the column.

02:50

Whether this person is off in a number of

02:57

interests or may be something wrong with this attribute.

03:02

Maybe what it says right here,

03:11

that age, for example,

03:15

is not the data right here.

03:17

So if you see a lot of mistakes in the column,

03:21

then you need to go to metadata,

03:24

which is data about data,

03:27

which is the description.

03:29

Of your data set and you need to re-check,

03:32

Is this the attribute that is supposed to be here?

03:35

Or may be all data is correct,

03:39

but the attribute is intra drawn.

03:41

Maybe it was not the age, maybe something else.

03:44

Completeness.

03:51

Of course, you will look if

03:53

you have some entries that are

03:56

empty or some data

03:58

that is missing, something's not recorded.

04:00

And subdata that was not available,

04:03

just screamed through it and decide if there is a lot,

04:05

for example, in a column or in a row,

04:08

then you may just want to delete

04:10

a lot of TED and missing in a particular column or row,

04:13

but we will talk about missing data.

04:18

So deleting is the worst that you can come up with.

04:20

Consistency.

04:27

Again, we'll talk about it a little bit later.

04:29

Some data was modified, some as not,

04:32

you wanted to have data to be written in the same form.

04:35

If it is modified and all should

04:39

be if it is not that all should be missing.

04:41

Is this data date.

04:44

So if you collected data or and if you're stating

04:48

that this is the current number for something,

04:52

then you want to make sure that

04:56

this current number is not dated 2010.

04:58

Okay.

05:02

Now, can you trust the data?

05:04

You can talk a lot about it, but right now,

05:09

it's just a simple question at this point.

05:14

If you see that someone entered this data

05:20

just because I promised

05:25

several extra points for completing this assignment.

05:27

Then District gardens.

05:30

If you think that somebody deliberately twisted data,

05:32

deliberately disregard, and so on.

05:36

If you can tell,

05:40

a lot of times we can not,

05:43

or sometimes there could be mistakes.

05:45

Or sometimes maybe you see

05:48

data that is coming from a lot of different sources.

05:51

And you know that this

05:54

particular source cannot be trusted.

05:56

So I'll go to disregard the standard.

05:58

When you Google wanted to,

06:01

to find answers to your questions.

06:03

I'm sure that you look at

06:06

the sources that this information is coming from.

06:08

And you already know some sources that you always trust.

06:12

In, some sources that you think, okay.

06:16

There could be read, they answer could be wrong,

06:20

wider with my time.

06:22

And so the same is going to happen here.

06:24

Let's if you know that you can trust

06:27

or you should not be trusted source.

06:30

Sometimes you may be unaware and use data that

06:32

is incorrect and all your study is going to

06:38

come out to indirect band.

06:41

So you may get very upset institution

06:46

when you publish something.

06:50

And then somebody will point out that,

06:51

you know, you shouldn't have trusted.

06:53

The last question, can you understand,

06:56

can you interpret the data?

07:00

Or for example, this metadata,

07:02

which is data about your data,

07:05

the explanation may be is missing

07:07

and you have no idea how this numbers.

07:09

You have the name of the attribute,

07:12

but you can't make sense of this numbers.

07:14

So don't try to analyze

07:17

something that you don't understand.

07:18

Or maybe this category is talking about some attribute.

07:20

Maybe it is related

07:24

to the subject that you did not understand.

07:27

Maybe this reliable data may be it is cleaned data,

07:30

complete data and so on.

07:34

But you don't understand what it means.

07:35

Read about it.

07:39

Ask professional, ask someone who understands.

07:41

Don't, don't touch data that you don't have clue above.

07:44

Because you may end up in a very funny story.

07:48

Berkeley's withdrawn.

07:52

Okay, right here, I've put some studies or

07:55

some ideas about data

08:01

clean interests for emission reduction can look later,

08:03

but we will be covering them.

08:06

Data cleaning. We start with data cleaning.

08:08

You will see this words use later on.

08:12

Sometimes they're used interchangeably if you want,

08:16

you can look into it

08:18

more Data Munging or some are st, one dream.

08:20

I've heard monk and more and data wrangling.

08:24

So these are slightly difference a different

08:27

to when not going to talk much about it.

08:31

He will, if you continue

08:35

with Data Science, if you're good to go to,

08:36

to data science, but just understand

08:39

they're dealing with data processing.

08:41

It is some sort of cleaning and transforming data.

08:44

Alright? Data cleaning.

08:49

And we'll talk about filling

08:52

in missing values, smoking noisy data.

08:55

Some outliers,

09:00

inconsistency, Incomplete, incomplete data.

09:03

There are no attribute values.

09:12

There are no some attributes that you are interested in.

09:16

In any case, you have data that is missing.

09:23

What are you going to do if you have

09:29

some incomplete data? What do you think?

09:33

What do you think? You ideas you'd have incomplete data.

09:51

The data is complete

09:58

without those points that have missing data,

10:00

you just run the analysis that requires the missing data,

10:02

ignore, ignore,

10:06

ignore the whole rows, right?

10:14

One way of dealing with Ignore the whole row,

10:19

if you have a lot of missing data.

10:26

You don't want to do that.

10:29

So you see, for example,

10:39

a student, if you collect the data from,

10:44

let's say 10 thousand students,

10:48

you see maybe five or seven of

10:50

them didn't answer half of the questions.

10:54

You may say, Well,

10:57

maybe are just disregard the surveys from those students.

11:00

It's possible, it's possible you don't want to,

11:07

to live the whole attribute though.

11:11

For the rows.

11:12

It's possible for the columns

11:14

or for the participants, for the entries.

11:16

It's possible for the attributes.

11:20

For the variables.

11:23

You don't want to delete

11:26

the whole variable, the whole column.

11:28

In this case download speed because some data is missing.

11:31

Now, this is the last thing that you're going to do.

11:34

Delete something from your data set.

11:38

And you will answer the question, why in a minute.

11:40

Now sometimes you may just want to fill

11:44

the missing values with the average, right?

11:48

Sure. Interested.

11:52

You may want to show with the average Yeah.

11:54

Possible.

11:57

Possible. You may want to fill it with the average.

11:58

So right now we have two ideas.

12:01

First, delete and second.

12:05

Replace with an average or just

12:09

include an average right

12:12

here instead of the missing data.

12:14

It's just, it's not just for the missing data.

12:16

For example, the download

12:18

speed right here by mistake we had minus 20.

12:20

It's certainly wrong data, right?

12:24

You don't expect minus 20 to be here.

12:27

Although some time was between t, I'm not sure.

12:30

But in any case,

12:35

it's not just when it's empty, unique to screen.

12:38

And see EBITDA belongs

12:41

to the range that you expect it to below.

12:43

Note, so you may fill it in with some value.

12:48

Now, why wouldn't you want to

12:52

delete this data if some values are missing?

12:56

Why wouldn't you want to just delete those roles?

13:09

Can still be useful, right?

13:17

It can still be useful, but give me an example.

13:20

We will study will go wrong.

13:23

It can be useful,

13:28

but you'll be absolutely wrong. You skip those.

13:30

Think about one of the examples that sometimes

13:52

Given in the books, there was

14:00

a study and it was related to the weight,

14:02

to something about overweight people.

14:06

Overweight people tend not to want to give their weight.

14:09

So those people who suite was

14:15

above a particular number

14:17

two where they thought they were overweight.

14:22

A lot of those people just missed

14:24

the values where the question was above their weight.

14:26

So the remaining values where given a normal weight,

14:30

most of the missed values were related to the hybrid.

14:35

That means the study was weight-related.

14:41

And we miss on all this tight high values,

14:45

then the study is just not going to include any of

14:49

the overweight people and only people

14:53

with normal or below no rock.

14:56

We too are going to participate in this study.

14:59

The whole analysis is going to be skewed.

15:02

What else may happen if you delete missing values?

15:07

Or when it may happen?

15:24

It may happen when the question,

15:31

for example, you can come up with your own examples.

15:34

When the question is asked wrong.

15:38

For example, your driver's license.

15:41

And Eve viscometers and listen,

15:47

maybe they're missing for a reason.

15:51

Maybe person does not have a license.

15:54

So we are going to remove this rows

15:57

just because the person does not have drivers, nurses.

16:01

Always when you ask this questions and give

16:04

some opportunity for the people

16:08

that cannot answer this question

16:13

with what you're asked to somehow answer.

16:16

So you can always include an I don't have it.

16:18

Or you can look at the Age and infirm.

16:23

So this missing values can tell you more

16:27

than those values that are there and so forth.

16:32

So a lot of times

16:36

those missing values are not there for a reason.

16:38

Analyze first.

16:43

Why aren't they there?

16:45

One question.

16:49

And this is the data about that houses,

16:52

the prices and their location,

17:00

their price, number of bedrooms, bathrooms, size.

17:03

And so what end price right here?

17:06

First idea was to replace

17:10

a missing value with some average

17:14

or mean, which is a good idea.

17:16

And a lot of times it is used.

17:18

What is going to happen if it is used here?

17:21

In this data set for the price.

17:27

It's not reasonable to do SLA?

17:31

Correct.

17:34

Because but the value of

17:35

the house depends on many parameters.

17:38

And if you have the highest values, house,

17:41

valued, house whose price is missing,

17:46

and you are going to just simply

17:49

replace it with an average.

17:50

It's going to mislead.

17:52

And several of those you're going to do,

17:55

it's going to be very misleading.

17:57

So in this case,

17:59

what you may want to do is to cluster,

18:01

to cluster your data,

18:06

to group your data,

18:07

and have similar houses,

18:08

similar valued houses to be in a particular group.

18:12

In this case, you can represent

18:17

the missing value with the average for that group.

18:20

With the average for that particular group.

18:26

Somebody mentioned to.

18:30

You can use a regression.

18:32

You can use regression to fit the value.

18:36

We're going to work on it later on.

18:40

It's going to be in a couple of weeks.

18:42

And at that point you will understand what it means.

18:43

Right now let me show you,

18:47

just show you a picture for example.

18:49

So for example, you have

18:55

an idea about a lot of data points.

18:57

You want to find this red light, red line.

19:00

We will learn how to do it.

19:05

And if you have missing values, now it's easy.

19:07

I have a missing value.

19:11

For example.

19:13

For example, this is,

19:14

I don't know what it can represent.

19:16

That can't be negative.

19:18

Let's say some coordinates.

19:20

It's the easiest that I can think about.

19:22

You have a coordinate X

19:24

or you know who the status come from and,

19:27

but you don't have white.

19:31

So you just look at your line of best fit and you'll find

19:33

the point on the line which will tell you what y is.

19:36

You know, what x is,

19:41

you will know what is.

19:44

The line of best fit is a great tool and sometimes it is

19:47

used for that purpose to replace the missing data.

19:52

However, it is sometimes tedious to do so.

19:57

If you have a lot of data,

20:00

if you have a lot of parameters.

20:02

So it depends on how important it is for,

20:04

you know, noise.

20:07

What do we do with noise?

20:12

Suppose you have a random error variance

20:15

in a measured variable.

20:19

Of course, you would want to access this values.

20:22

If it is a huge data set,

20:26

it's very difficult to do it manually.

20:29

If it is a small data set,

20:31

maybe you want to do manually.

20:33

In either case, do not just assume

20:35

that those are because of some error.

20:39

Maybe not, maybe not,

20:44

and you will miss a very important information.

20:48

Okay? Now, never just drunk data.

20:54

Or disregard outliers for your study.

21:01

You may want to remove the outliers, but first,

21:06

analyze them before you

21:09

or how do we

21:10

deal with noisy data?

21:18

The easiest is been thinning.

21:21

You have lots of data.

21:26

That is, for example,

21:29

that is all over the place everywhere.

21:32

Suppose there is a time series and

21:37

you want to see what's happening every day.

21:41

And if you remember, I was showing this data uncovered.

21:44

So in that case,

21:50

each data point was showing

21:53

how many cases there were diagnosed was covered in date.

21:57

So you may have some days, for example,

22:03

Saturday and Sunday, where the numbers are going lower.

22:07

Does it mean that the numbers went down?

22:12

Note it doesn't.

22:14

Maybe they were just thought

22:16

recorded reported on Saturday,

22:18

Sunday because it was a weekend.

22:20

And then on Monday and Tuesday, we're searching.

22:22

Is it because a lot

22:26

of a lot more people who are diagnostic?

22:28

Do we have another case

22:31

when the numbers of rapidly growing up?

22:34

Not necessarily. Maybe it's because on Saturday,

22:36

Sunday, they were undiagnosed and then on Monday,

22:39

Tuesday, they came out to

22:43

be the numbers came out to be hard because of that.

22:45

So in that case,

22:51

they were smoothing the data by using a day average,

22:53

70 average, which means that at each data point,

22:58

they looked back seven days back.

23:02

And the curve was

23:05

smooth and your data in what's showing the trend.

23:10

In that case. In case of Benin,

23:14

you will say, alright,

23:17

instead of looking at data Monday,

23:20

Monday, Tuesday, Wednesday, Thursday,

23:26

I will just put the whole week in one bin.

23:28

And that is going to be a bit average for the week.

23:35

And that is going to be my next week.

23:40

I'm sorry, it must be the same.

23:43

And that is going to be my next week is

23:48

the average was higher.

23:51

And then it's going to be my next week.

23:53

And another week and another.

23:56

Okay?

24:00

This is binning.

24:01

How can you decide

24:03

on what is going to be the value that represents the bin?

24:09

Well, it can always be the average.

24:14

It can be, it can be the first day represents or

24:17

the last date in our COBIT case represents the bin.

24:22

So there are different ways of doing so,

24:27

but Benin is very simple and it's very helpful,

24:30

very simple and very helpful.

24:35

What you do, you have data,

24:39

for example, right here.

24:42

This is priced in dollars unique to ordered first,

24:43

and then you partition it into groups of the same size.

24:48

Alright?

24:54

And the last one, whenever is

24:56

left inside our particular group,

24:57

you decide what to do.

25:00

So you can smooth it.

25:03

You can smooth it by means is it's done here.

25:07

You can smooth it by being boundaries.

25:12

Have been founders either in our covert case,

25:14

the first day is going to

25:17

be the height of it or the last day,

25:20

whatever you choose or the mean.

25:23

And we looked already at dealing

25:27

with noise right here by using the regression.

25:33

So, I'm sorry, you're dealing

25:38

with missing data by using the regression,

25:40

but also we can deal with noise

25:42

the same way by using regression.

25:44

Likewise, you can use clustering.

25:49

We already mentioned it when

25:53

we were talking about to our missing data.

25:55

So if, you know,

25:58

if you defined the groups that your data belongs to,

25:59

for example, these are low.

26:04

The houses that are on the lower side,

26:07

price-wise, medium and the largest value?

26:10

Well, should we probably that's the largest.

26:14

The medium and the lower, Something like that.

26:17

If you can cluster or partition into groups.

26:20

And then you have some outliers.

26:25

If you think that for those outliers

26:28

the price needed just mistakenly recorded the way it was.

26:31

And all the parameters,

26:36

all the other attributes for those houses are similar,

26:38

then you may want to include it into the same cluster.

26:41

Alright. I'm going to stop right now.

26:46