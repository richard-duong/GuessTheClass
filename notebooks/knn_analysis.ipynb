{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import sys\n",
    "import pandas as pd\n",
    "sys.path.append(\"../\")\n",
    "from YouReader.Reader import Reader\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer,TfidfTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from pprint import pprint\n",
    "import time\n",
    "\n",
    "# pandas settings\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('max_rows', None)\n",
    "\n",
    "# Constants\n",
    "TOKEN_PATTERN = r\"[^\\s]+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SnowballStemmer Override for  TfidfVectorizer\n",
    "# referenced from https://stackoverflow.com/questions/36182502/add-stemming-support-to-countvectorizer-sklearn\n",
    "\n",
    "english_stemmer = SnowballStemmer(\"english\")\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super().build_analyzer()\n",
    "        return lambda doc: ([english_stemmer.stem(w) for w in analyzer(doc)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Loading Data from Save</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1151 captions from save.json\n",
      "\n",
      "\n",
      "subject\n",
      "BIOL      82\n",
      "BUS      135\n",
      "CS       239\n",
      "ENGL      65\n",
      "MATH     269\n",
      "POLI     137\n",
      "PSYCH     70\n",
      "Name: link, dtype: int64 \n",
      "\n",
      "\n",
      "This took 1.30 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "\n",
    "reader = Reader()\n",
    "count = reader.load_captions(\"../data/save.json\")\n",
    "df = reader.to_dataframe()\n",
    "subject_totals = df.groupby(\"subject\")[\"link\"].count()\n",
    "\n",
    "\n",
    "print(\"Loaded\", count, \"captions from save.json\\n\\n\")\n",
    "print(subject_totals, \"\\n\\n\")\n",
    "print(\"This took\", \"{0:.2f}\".format(time.time() - start_time), \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Vectorize and stem the corpus</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the custom stemming vectorizer\n",
    "stem_vectorizer = StemmedTfidfVectorizer(min_df=.000001, max_df=1, stop_words=\"english\", token_pattern=TOKEN_PATTERN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This took 62.68 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# fit data to all transcripts to generate vocabulary / features\n",
    "stem_fit_vectorizer = stem_vectorizer.fit(df[\"clean\"])\n",
    "\n",
    "\n",
    "print(\"This took\", \"{0:.2f}\".format(time.time() - start_time), \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Transform Training and Test Datasets</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of testing dataset is:  96\n",
      "Size of training dataset is:  901\n",
      "This took 0.92 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# generate training and testing sets\n",
    "all_keys = list(df.index.values)\n",
    "all_subjects = df[\"subject\"].unique()\n",
    "\n",
    "test_data = []\n",
    "test_labels = []\n",
    "train_data = []\n",
    "train_labels = []\n",
    "\n",
    "# partition testing dataset to be 1/5, and training dataset to be 4/5 of original\n",
    "for subject, total in zip(subjects, subject_totals):\n",
    "    subject_keys = [key for key in all_keys if df.loc[key][\"subject\"] == subject]\n",
    "    subject_clean = [df.loc[key][\"clean\"] for key in subject_keys]\n",
    "    subject_subject = [df.loc[key][\"subject\"] for key in subject_keys]\n",
    "    \n",
    "    test_size = total // 10\n",
    "    test_data.extend(subject_clean[:test_size])\n",
    "    test_labels.extend(subject_subject[:test_size])\n",
    "    train_data.extend(subject_clean[test_size:])\n",
    "    train_labels.extend(subject_subject[test_size:])\n",
    "\n",
    "    \n",
    "print(\"Size of testing dataset is: \", len(test_data))\n",
    "print(\"Size of training dataset is: \", len(train_data))    \n",
    "print(\"This took\", \"{0:.2f}\".format(time.time() - start_time), \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset has the shape:  (901, 30271)\n",
      "Testing dataset has the shape:  (96, 30271)\n",
      "This took 65.70 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# transforms datasets into feature vectors for ML analysis\n",
    "train_transform = stem_fit_vectorizer.transform(train_data)\n",
    "test_transform = stem_fit_vectorizer.transform(test_data)\n",
    "\n",
    "print(\"Training dataset has the shape: \", train_transform.shape)\n",
    "print(\"Testing dataset has the shape: \", test_transform.shape)\n",
    "print(\"This took\", \"{0:.2f}\".format(time.time() - start_time), \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>KNN Fitting and Prediction</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=3)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generates and fits classifier to training dataset\n",
    "classifier = KNeighborsClassifier(n_neighbors=3)\n",
    "classifier.fit(train_transform, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CS', 'CS', 'CS', 'CS', 'CS', 'CS', 'CS', 'CS', 'CS', 'CS', 'CS',\n",
       "       'CS', 'CS', 'CS', 'CS', 'CS', 'CS', 'CS', 'CS', 'CS', 'CS', 'CS',\n",
       "       'CS', 'CS', 'CS', 'CS', 'CS', 'CS', 'CS', 'CS', 'CS', 'CS', 'CS',\n",
       "       'CS', 'CS', 'CS', 'CS', 'CS', 'CS', 'CS', 'CS', 'CS', 'CS', 'CS',\n",
       "       'CS', 'CS', 'CS', 'CS', 'CS', 'CS', 'CS', 'CS', 'CS', 'CS', 'CS',\n",
       "       'CS', 'CS', 'CS', 'CS', 'CS', 'CS', 'CS', 'CS', 'CS', 'CS', 'CS',\n",
       "       'CS', 'CS', 'CS', 'CS', 'CS', 'CS', 'CS', 'CS', 'CS', 'CS', 'CS',\n",
       "       'CS', 'CS', 'CS', 'CS', 'CS', 'CS', 'CS', 'CS', 'CS', 'CS', 'CS',\n",
       "       'CS', 'CS', 'CS', 'CS', 'CS', 'CS', 'CS', 'CS'], dtype='<U5')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = classifier.predict(test_transform)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CS'], dtype='<U5')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = [\"political\"]\n",
    "sample_text_transform = stem_fit_vectorizer.transform(sample_text)\n",
    "classifier.predict(sample_text_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
