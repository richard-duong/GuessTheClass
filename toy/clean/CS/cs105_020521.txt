We
started
talking
about
loss
functions
And
you
understand
more
or
less
what
the
loss
function
means
In
order
to
train
our
model
we
need
to
understand
for
each
particular
set
of
parameters
how
good
we
are
at
the
values
that
we
want
to
predict
By
first
running
our
examples
on
the
training
data
We
are
running
it
on
the
data
that
can
insert
our
question
whether
our
prediction
is
correct
or
not
or
int
for
how
close
it
is
to
the
correct
value
We
collect
all
the
information
about
every
time
when
we
miss
the
correct
answer
or
every
time
when
we
compute
a
value
we
compare
it
with
the
correct
value
and
we
will
see
how
far
we
are
from
it
It
can
be
done
using
mean
squared
error
loss
function
We
looked
at
this
function
lost
time
which
way
we're
doing
it
We
find
the
difference
between
our
predicted
value
in
the
correct
value
Then
we
find
the
square
of
it
and
all
the
points
And
divide
by
the
number
of
points
Add
information
about
each
point
and
divide
by
the
number
of
points
This
is
going
to
be
our
loss
Once
we
compute
the
value
of
this
expression
is
going
to
be
our
loss
We
can
take
this
in
this
example
We
started
with
theta
equals
121314
And
so
one
moving
this
wave
we'll
find
the
point
with
the
smallest
loss
We
also
said
last
time
that
this
function
is
a
quadratic
polynomial
So
what
we
technically
what
to
do
We
wanted
to
do
the
minimum
of
this
function
We
want
to
find
the
minimum
of
this
function
How
can
it
be
done
It
can
be
done
by
finding
credit
So
what
we'll
find
the
derivative
and
we
find
where
this
derivative
is
equal
to
x2
By
checking
the
behavior
of
it
So
this
value
or
let's
say
to
the
left
of
the
valve
and
to
the
right
of
the
value
We
will
confirm
that
this
is
indeed
a
minimum
Maximum
right
We
don't
know
what
this
fun
not
this
particular
function
just
in
general
what
function
behaves
like
So
that's
how
we
confirm
that
the
extremum
isn't
minimum
Now
are
we
going
to
do
it
actually
quiescent
technical
person
does
everything
for
us
But
you
need
to
know
what
this
function's
perform
and
how
they
perform
what
they
do
exactly
For
that
reason
I
included
some
formulas
right
here
So
if
you
have
enough
of
a
background
in
calculus
you
can
go
through
those
If
not
at
least
you
understand
the
idea
of
the
process
Even
if
you
don't
look
at
the
formulas
you
understand
the
idea
of
this
process
What
is
shown
right
here
that
for
the
value
that
we're
looking
for
that
value
that
means
squared
error
is
going
to
give
us
is
the
mean
is
the
new
mean
of
y
mean
Apollo
And
I
told
you
that
we
want
to
know
about
this
mean
absolute
value
that
behaves
not
behaves
that
The
idea
is
the
same
We
look
at
each
given
data
point
and
find
the
distance
between
our
data
points
and
our
currently
predicted
value
And
we
compute
the
loss
function
that
is
going
to
be
the
sum
of
all
of
our
nonetheless
the
sum
of
all
of
our
errors
were
only
taken
an
absolute
value
All
it
of
course
we
cannot
afford
not
taken
absolute
values
because
if
we
have
values
with
different
signs
then
the
GUI
will
not
go
into
have
an
information
about
the
error
Because
negative
values
when
you
add
two
positive
values
will
reduce
the
number
of
fear
and
that's
what
we
want
to
avoid
And
of
course
we
divide
by
in
compute
the
sum
seven
divide
by
n
You
cannot
compare
the
values
themselves
of
this
function
And
this
function
because
that
is
going
to
be
square
root
of
that
is
going
to
be
an
absolute
value
So
obviously
the
values
are
not
comparable
sometimes
right
here
is
use
root
mean
squared
error
So
that
is
done
to
what
you
need
to
know
about
this
squared
error
in
absolute
value
here
When
do
you
want
to
use
and
what
you
need
to
think
about
when
you're
using
them
This
is
a
squared
error
and
this
is
an
absolute
error
But
they
are
they
are
very
close
not
the
value
wise
but
behavior
was
their
behavior
was
is
closed
closer
to
the
minimum
the
minimum
of
that
function
to
the
extremum
And
they
behave
very
differently
for
the
outliers
Well
not
necessarily
just
for
the
outliers
The
further
you
are
from
the
mean
and
median
values
which
don't
have
to
be
the
same
Obviously
The
more
is
the
difference
in
their
behavior
is
you
notice
the
squared
error
function
penalizes
much
more
for
those
values
that
are
further
from
the
mean
And
especially
for
outliers
Especially
for
outliers
It
gives
much
bigger
penalty
compared
to
the
linear
function
Just
think
about
it
this
way
1234
If
we
are
one
unit
apart
then
square
root
error
will
penalize
by
one
Absolute
value
will
penalize
by
one
If
we're
to
units
of
port
the
absolute
valuable
penalized
by
the
squared
penalized
by
four
So
you
can
see
that
well
you
can
understand
that
absolute
value
penalizes
linearly
And
of
course
squares
penalizes
this
by
taking
the
square
by
multiplying
by
itself
So
if
you
do
not
want
to
penalize
for
the
outliers
then
you're
going
to
use
the
absolute
value
If
you
want
to
pay
attention
to
those
values
that
are
further
than
he
wanted
to
take
a
squared
optimization
problems
A
lot
of
times
they're
just
using
a
squared
error
loss
function
A
lot
of
times
you
are
using
the
function
that
is
a
combination
of
the
two
In
a
second
We're
going
to
show
them
All
right
So
we
looked
at
the
squared
error
and
showed
that
it
is
mean
Absolute
error
is
giving
you
the
median
You
know
that
the
mean
and
the
median
are
not
necessarily
the
same
And
you
know
the
cases
when
you
would
like
to
use
the
mean
when
you
would
like
to
use
the
medium
The
Heber
loss
function
that
combines
the
two
combines
the
advantages
of
each
function
As
you
can
see
if
the
distance
Between
the
points
If
the
error
is
less
or
equal
than
some
value
then
you
are
using
the
squared
error
And
if
the
distance
is
greater
so
if
you
don't
want
to
penalize
as
much
for
the
extreme
values
for
the
values
that
are
forth
from
from
the
millimeter
then
you
can
use
the
absolute
the
absolute
error
function
But
as
you
can
see
there
are
some
parameters
for
right
here
that
allow
you
to
compare
the
values
otherwise
they
cannot
be
compared
How
do
you
choose
alpha
right
here
And
we're
not
going
to
talk
about
this
alpha
right
here
We'll
be
talking
about
some
gradient
descent
later
on
okay
Because
in
this
case
we're
not
going
to
look
at
Heber
lost
A
lot
of
times
in
machine
learning
Pickin
parameters
is
an
art
an
art
that
is
coming
from
your
experience
from
your
knowledge
from
your
knowledge
of
data
So
a
lot
of
times
when
I
say
an
art
it
doesn't
mean
that
it
is
just
coming
from
nowhere
You
close
your
eyes
and
you
think
that's
a
pretty
number
What
I
mean
is
there
is
no
formula
that
will
tell
you
alpha
is
this
Okay
We
will
talk
about
this
perimeters
So
I
don't
want
to
just
to
rush
through
it
we
will
talk
about
the
parameters
gradient
descent
So
when
we
weren't
to
loss
functions
we
said
let's
pick
theta
equal
to
12
then
13
then
14
and
so
on
How
do
we
know
where
to
start
How
do
we
know
how
big
those
steps
should
be
12-13
maybe
I
should
have
taken
1215
maybe
I
should
have
taken
12
512
and
so
on
How
do
I
make
the
decision
of
the
next
theta
For
our
tips
dataset
it
is
a
relatively
small
set
We
know
that
the
minimum
values
that
the
maximum
values
then
basically
we
can
just
run
from
the
minimum
to
the
maximum
value
Now
typically
it
can
be
from
a
0
to
a
100
If
we'll
look
in
on
a
graph
because
that
was
a
percentage
rate
When
we're
looking
at
a
graph
we
can
estimate
And
we
did
it
last
time
Well
that
was
somewhere
between
10
20%
So
we
can
try
beta
between
1020
But
obviously
we
don't
want
to
depend
every
time
on
a
particular
dataset
On
us
seem
a
visual
information
about
this
dataset
and
for
us
knowing
where
exactly
this
value
can
be
So
there
were
first
a
lot
of
assumptions
made
for
our
tips
dataset
And
second
we
did
a
lot
of
work
before
we
even
pick
That
theta
that
was
12
You
didn't
think
about
it
right
Because
it
was
neutral
Here's
a
dataset
Let's
look
at
the
graph
It
looks
like
this
is
what
we
can
do
But
what
if
we
don't
have
that
information
or
the
data
set
is
large
Or
if
we
wanted
to
create
some
packages
that
are
not
going
to
be
data
dependent
How
can
we
solve
this
problem
in
general
So
suppose
we
didn't
know
that
It's
good
to
start
at
121012
doesn't
matter
Let's
see
What
did
we
do
We
created
at
graduate
Remember
we
looked
after
looking
at
1214
and
so
on
We
said
let's
look
at
the
derivative
of
this
function
So
we
looked
at
this
line
at
the
behavior
of
this
line
which
is
tangent
to
this
particular
curve
At
this
point
Let's
see
how
this
line
behaves
All
right
It
was
we
were
moving
slower
than
this
line
would
continue
Heaven
Here
we
go
Right
angle
The
angle
of
having
an
acute
angle
right
here
this
acute
angle
But
if
we
jumped
too
far
then
the
angle
would
have
changed
to
the
opposite
It
would
have
changed
to
the
tubes
depending
on
which
direction
we're
talking
about
But
this
sign
would
have
changed
the
direction
Would
it
change
That
means
what
would
it
mean
by
the
way
What
would
it
mean
that
this
angle
changed
So
we
found
the
derivative
that's
a
function
right
We
found
the
value
We
found
that
this
angle
changed
to
that
angle
What
would
it
mean
You
just
subtract
the
gradient
What
would
it
mean
if
the
accused
changed
to
acute
or
vice
versa
That
we
missed
the
point
that
we
missed
that
point
and
we
need
to
go
back
Look
the
line
goes
here
here
here
here
at
this
point
it's
parallel
to
x
axis
And
then
the
angle
changed
So
if
we
go
from
here
to
here
If
we
took
too
big
a
step
then
we
miss
this
point
If
we
went
from
12
to
17
for
example
then
we
missed
the
point
and
the
next
value
of
theta
should
not
go
up
It
should
go
back
It
should
go
down
Is
that
clear
Because
you
you
learned
some
analysis
or
calculus
right
Did
you
learn
it
Because
if
not
you've
got
to
tell
me
right
now
I'm
not
sure
but
if
you
continue
to
just
subtract
the
gradient
then
it
shouldn't
matter
It
matters
which
direction
we
go
We
can
continue
moving
181920
and
is
the
one
we're
going
to
be
somewhere
else
not
where
we're
supposed
to
be
Let's
look
at
the
following
example
How
we
are
going
to
give
an
optimization
Our
goal
is
to
find
a
local
minimum
Now
all
differentiable
function
I
should
be
able
to
find
a
derivative
of
this
function
right
The
gradient
descent
is
good
to
find
a
local
minimum
If
this
function
has
several
minimum
or
that
are
the
same
or
different
And
I'm
somewhere
here
I'm
going
to
find
this
minimum
I'm
going
to
move
right
here
If
I
make
huge
step
I
will
end
up
being
here
they
are
somewhere
else
But
in
any
case
whatever
the
minimum
I
find
it's
just
a
local
minimum
I
don't
know
if
it's
global
as
well
or
not
Alright
That's
my
function
X
plus
five
squared
I
take
a
point
on
the
curve
at
some
point
that
belongs
to
the
upper
right-hand
I
find
the
gradient
of
the
function
which
is
going
to
be
the
derivative
of
this
expression
Two
X
two
X
plus
52
times
X
plus
five
That's
just
times
I
can
find
the
value
At
this
point
What
I
want
to
know
is
to
setup
some
learning
rate
Again
it
comes
often
it
comes
from
the
experience
But
a
lot
of
times
I'm
just
going
ahead
of
time
But
a
lot
of
times
with
taken
for
the
learning
rate
some
power
of
ten
So
could
be
0
1
could
be
0
01
could
be
0
0
0
one
So
ten
to
the
negative
12345
something
like
that
Now
if
we
are
afraid
of
skipping
the
minimum
then
we're
taken
a
realist
small
value
If
we
know
anything
about
our
function
we
may
know
what
is
small
Give
me
a
second
and
we'll
get
back
to
this
question
But
if
we
don't
know
anything
and
if
we
want
just
to
go
through
the
function
will
then
sometimes
we
want
to
take
a
little
larger
of
this
felt
you
and
see
if
there
are
other
minimum
in
this
functions
because
it
made
bring
me
to
the
different
minimum
from
here
to
there
For
example
in
sometimes
we
alternate
We're
not
going
to
go
through
all
of
this
We're
just
going
to
work
on
the
local
minimum
for
now
Ok
we're
only
going
to
try
to
find
this
minimum
That's
all
we
have
sit
in
up
the
learning
rate
which
is
0
01
And
we
will
see
what
is
going
to
happen
If
I
go
if
I
go
to
the
left
with
this
rate
what's
going
to
happen
Where
am
I
going
to
beam
Why
would
I
go
to
the
left
Because
the
direction
of
the
tangent
tells
me
where
my
local
minimum
is
I
am
I
am
trying
to
avoid
calculus
at
least
some
calculus
you
need
to
understand
Why
did
he
choose
0
01
Just
for
this
example
A
lot
of
times
we
may
just
want
to
choose
0
01
or
0
01
These
are
just
some
values
that
are
common
to
choose
Will
still
say
you
got
that
question
I
can
explain
right
now
Of
course
the
smallest
value
we
choose
the
less
is
the
probability
that
we
miss
this
minimum
However
if
our
value
is
too
small
then
what's
going
to
happen
If
our
value
is
too
small
it
will
take
more
iterations
right
So
it
will
take
a
very
long
time
I'll
be
just
sitting
right
here
and
moving
very
very
very
slowly
because
the
learning
rate
is
the
speed
with
which
I
learned
So
if
I
have
a
very
slow
speed
then
I
learn
slow
So
sometimes
I
may
want
to
see
nothing
is
happening
right
here
Let
me
increase
the
speed
Alright
so
how
it
works
how
it
works
We
have
to
perform
several
iterations
Here
At
each
iteration
my
goal
is
well
my
ultimate
goal
is
to
reach
this
minimum
right
But
each
iteration
is
going
to
give
me
the
next
point
right
here
is
going
to
happen
X1
or
the
next
point
is
the
point
that
I'm
looking
for
in
iteration
one
is
going
to
be
x
0
minus
something
How
big
is
this
something
It's
going
to
be
learning
rate
times
d
y
by
d
x
evaluated
at
x
0
So
our
previous
point
is
negative
three
minus
our
learning
rate
times
evaluate
dy
over
dx
at
minus
three
And
you
will
need
between
now
and
next
Wednesday
you
will
need
to
review
a
little
bit
of
trigonometry
sine
cosine
and
tangent
and
a
little
bit
of
calculus
the
derivative
and
what
it
means
Okay
So
that
should
be
enough
This
way
I
am
going
to
find
the
next
point
X1
is
the
next
point
right
This
is
the
next
point
I
am
moving
to
this
point
and
I
perform
the
next
iteration
which
is
going
to
look
exactly
like
the
previous
iteration
It's
just
going
to
start
at
a
different
point
So
we're
going
to
we're
going
to
perform
the
next
iteration
and
find
the
next
value
the
next
value
and
so
on
Next
when
am
I
going
to
fit
Media
should
stay
right
here
When
am
I
going
to
finish
I'm
going
to
finish
when
the
values
almost
don't
change
Usually
when
we
say
we
change
we
stop
when
when
theta
is
the
same
Well
good
prediction
But
it's
never
going
to
be
the
same
So
you'll
sit
enough
some
threshold
and
your
say
when
the
change
between
the
previous
step
and
the
next
step
is
such
And
you
decide
for
yourself
what
you
want
it
to
be
It
depends
on
your
function
Sometimes
sometimes
you're
just
saying
well
okay
0
01
just
to
set
up
some
threshold
So
that's
the
value
When
it
stabilizes
Samuelson
alright
I'm
done
here
I
want
to
stop
my
album
from
running
Or
sometimes
you
just
want
to
say
I
perform
so
many
iterations
But
in
that
case
you'd
better
know
at
least
approximately
the
behavior
of
the
function
Okay
So
we
start
with
a
lot
of
times
If
we
don't
know
anything
where
to
start
then
we're
just
saying
let
theta
be
0
That's
it
I
have
no
idea
where
to
start
Let
it
be
0
This
is
some
calculus
If
you
wanted
to
go
through
this
calculus
I
would
think
you
should
If
you
have
enough
knowledge
If
you
don't
you
will
not
have
as
deep
an
understanding
of
the
graded
descent
and
the
regression
However
that's
not
going
to
change
Your
understanding
of
the
process
gets
so
And
if
you
worry
at
this
point
that
is
going
to
bring
down
your
great
then
no
So
I
am
trying
to
accommodate
I'm
trying
to
accommodate
as
much
as
possible
all
students
in
this
class
If
I
just
remember
if
I
skip
through
the
slides
it
is
some
additional
information
not
because
I
don't
have
time
to
corporate
those
just
because
sometimes
I
think
it
may
be
difficult
for
some
students
that
are
on
their
second
year
but
it
may
be
useful
for
some
other
students
So
understand
that
if
you
feel
like
it
you
go
over
it
If
you
don't
then
it's
okay
if
you
skip
it
Okay
So
what
is
valued
at
Great
question
what
is
this
value
What
is
this
value
right
Now
Like
I
said
before
this
perimeters
that
we
cannot
setup
in
a
scientific
way
There
are
some
of
those
that
we
need
to
either
decide
on
from
our
knowledge
or
just
say
okay
let
it
be
this
and
see
how
it
behaves
Okay
So
for
this
learning
rate
to
the
rests
on
common
values
to
use
like
0
01
is
a
very
common
value
But
if
you
see
that
you
are
not
moving
very
far
you
may
want
to
increase
it
If
you
see
that
you're
jumps
are
very
large
then
you
may
decrease
it
Now
you're
not
going
to
worry
about
it
in
this
class
If
you
get
to
machine
learning
class
later
on
that
maybe
just
maybe
you
will
Sometimes
you
just
want
to
look
at
different
values
You
want
to
run
your
algorithm
by
Saint
Alright
let
me
try
to
run
this
algorithm
with
alpha
being
0
01
or
alpha
being
0
001
Now
a
common
strategy
is
to
start
with
a
little
larger
outfits
And
then
when
you
see
that
you
are
approaching
your
minimum
then
you're
making
the
values
of
alpha
smaller
so
you
don't
miss
it
So
this
way
you
are
not
moving
too
slow
but
on
the
other
hand
you're
not
going
to
miss
it
So
depending
on
how
large
the
steps
that
you
make
in
right
here
your
change
in
your
learning
rate
You're
not
going
to
deal
with
it
in
this
class
but
this
is
just
if
you're
interested
how
alpha
is
big
So
we
may
want
to
start
with
the
larger
alpha
and
then
make
it
smaller
when
we
approach
him
Alright
And
basically
this
is
what
we
just
discussed
This
is
the
same
as
I
showed
you
on
that
example
For
the
parabola
I
showed
that
simple
examples
so
you
don't
get
scared
right
away
I've
seen
this
formulas
So
you
can
see
this
is
our
previous
style
A
bit
learning
rate
and
evaluating
the
gradient
descent
at
that
point
at
a
particular
point
So
that
is
going
to
give
us
the
value
for
the
next
iteration
Alright
more
or
less
And
this
is
just
a
little
bit
more
But
I'm
looking
at
you
and
I
don't
know
I
probably
should
not
date
worrying
about
those
As
as
I
said
before
you
can
only
find
a
local
minimum
by
using
gradient
descent
if
you
jump
to
four
right
here
for
example
you
are
going
back
and
you'll
find
in
this
local
minimum
But
if
you
jump
in
too
far
then
you
may
just
find
this
one
In
any
case
In
any
case
it's
going
to
find
just
the
local
power
Of
course
you
can
use
some
calculus
or
analysis
to
first
see
the
behavior
of
your
function
And
then
you
will
know
more
what
to
use
the
gradient
for
But
it's
out
of
scope
of
this
class
So
what
we
are
going
to
do
is
we're
going
to
just
find
this
particular
minimum
If
the
function
has
only
one
minimum
then
gradient
descent
is
perfect
for
the
case
because
no
matter
what
you
do
you're
going
to
find
your
local
minimum
Well
no
matter
what
you
do
if
you
know
what
you're
doing
of
course
Because
if
you
set
up
a
large
ALS
heart
and
he
don't
change
it
then
you
can
just
moving
back
and
forth
With
itself
Okay
so
I'm
going
to
stop
talking
about
loss
functions
right
now
Our
next
lecture
is
going
to
be
on
regression
We
will
be
talking
about
regression
analysis
And
this
will
be
the
first
technique
that
will
be
a
candidate
for
your
project
Three
techniques
for
your
project
are
going
to
be
regression
Case
near
k
nearest
neighbors
K-means
clustering
and
hierarchical
clustering
So
you
will
need
to
choose
three
either
out
of
this
four
But
that
we
will
study
in
class
or
out
of
those
that
you
know
if
you
took
if
you
took
machine
learning
class
maybe
you
know
something
else
some
other
techniques
I
don't
mind
if
you're
using
those
Believe
me
I
know
all
of
them
so
well
Not
all
of
those
that
exist
but
certainly
those
that
he'll
learn
in
machine
learning
class
So
I'm
fine
with
it
If
you
want
to
run
something
for
your
project
you
will
need
to
find
some
dataset
is
suitable
for
you
running
this
techniques
So
do
not
rush
into
findings
some
datasets
that
you
just
like
because
it's
possible
that
this
is
an
interesting
data
interesting
dataset
Interesting
question
but
you
cannot
perform
this
particular
techniques
on
this
on
this
dataset
So
we
start
to
look
him
he
Mr
thinking
about
it
But
if
you
don't
it
is
true
You
do
not
have
to
I
just
had
some
questions
about
the
final
project
so
I
wanted
to
at
least
to
get
closer
to
what
it's
going
to
be
That
And
also
we're
going
to
look
a
little
bit
later
at
the
predictors
If
of
course
you're
using
Amazon
or
something
like
that
to
purchase
something
from
or
to
watch
movies
or
to
play
games
You
know
when
the
attrition
one
product
you
see
some
predictions
about
the
other
products
that
you
may
want
to
purchase
by
play
test
of
anything
So
I
will
show
you
how
you
can
possibly
construct
build
such
predictors
And
if
you
want
to
have
your
project
to
debt
that's
fine
too
But
it
may
be
a
little
bit
more
difficult
But
if
you're
interested
certainly
you
can
do
that
and
you
can
always
have
it
on
your
resume
That's
interesting
too
And
also
we're
going
to
perform
some
text
analysis
If
you
want
to
work
on
text
analysis
gets
fine
with
me
as
well
So
basically
this
project
is
pretty
open
Again
You
can
choose
what
you
are
interested
in
But
within
the
material
that
we
study
in
this
class
or
something
similar
that
you
may
have
learnt
from
different
class
Now
I
don't
advise
you
to
start
learning
these
techniques
If
you've
never
learned
them
before
Because
it's
going
to
take
a
lot
of
time
and
you're
still
go
into
the
most
likely
shake
it
on
them
So
I
would
suggest
take
something
that
we
either
doing
this
class
or
you
learned
before
This
is
my
suggestion
I
wouldn't
mind
you're
learning
something
new
which
is
always
great
But
I
understand
that
when
you
have
only
several
weeks
for
your
project
it
may
not
be
reasonable
Don't
set
goals
that
taking
her
not
to
reach
Okay
