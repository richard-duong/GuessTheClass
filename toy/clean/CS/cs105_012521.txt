And
this
is
exactly
what
you're
going
to
do
after
you
collect
your
data
Whether
whichever
type
of
data
collection
you
are
using
whichever
type
of
data
you
are
using
you
will
have
to
deal
with
some
preprocessing
more
or
less
That
depends
on
data
that
depends
on
the
task
that
you
are
going
to
perform
In
a
lot
of
cases
data
preprocessing
in
a
very
long
process
Sometimes
sometimes
scientists
say
about
80%
of
time
that
you
spend
on
your
projects
is
taken
by
data
pre-processing
So
be
ready
that
data
is
not
going
to
come
to
you
the
way
you
want
to
see
it
You
will
have
to
work
on
it
before
you
even
start
to
running
any
of
the
tests
that
you
learn
in
this
class
The
first
step
is
data
cleaning
then
data
integration
integration
transformation
And
you're
going
to
go
through
data
reduction
We
will
talk
about
each
process
and
Ben
you
will
go
through
all
of
these
processes
during
the
next
lab
when
you
collect
your
data
from
service
Believe
me
data
that
you
collect
is
going
to
be
worse
than
any
data
that
is
already
published
Because
what
is
published
is
already
a
little
bit
cleaner
a
little
bit
brushed
more
or
less
But
it
is
in
some
way
it's
already
taken
care
of
You
are
going
to
get
data
as
raw
as
it
gets
Good
hands
dirty
First
When
you
see
your
data
check
data
quality
Just
screen
looks
through
accuracy
Sometimes
you
can't
tell
whether
it
is
accurate
or
not
For
example
a
simple
question
above
to
the
age
Whether
you
needed
for
the
Serbian
nod
but
a
simple
question
And
if
you
see
that
somebody
put
negative
five
or
a
130
your
already
questioning
this
numbers
right
So
something
may
be
inconsistent
with
the
information
that
you
expect
it
to
get
that
you
see
immediately
it
is
drawn
What
you
need
to
do
at
this
time
you
need
to
Usually
you
may
want
to
check
the
row
and
the
column
Whether
this
person
is
off
in
a
number
of
interests
or
may
be
something
wrong
with
this
attribute
Maybe
what
it
says
right
here
that
age
for
example
is
not
the
data
right
here
So
if
you
see
a
lot
of
mistakes
in
the
column
then
you
need
to
go
to
metadata
which
is
data
about
data
which
is
the
description
Of
your
data
set
and
you
need
to
re-check
Is
this
the
attribute
that
is
supposed
to
be
here
Or
may
be
all
data
is
correct
but
the
attribute
is
intra
drawn
Maybe
it
was
not
the
age
maybe
something
else
Completeness
Of
course
you
will
look
if
you
have
some
entries
that
are
empty
or
some
data
that
is
missing
something's
not
recorded
And
subdata
that
was
not
available
just
screamed
through
it
and
decide
if
there
is
a
lot
for
example
in
a
column
or
in
a
row
then
you
may
just
want
to
delete
a
lot
of
TED
and
missing
in
a
particular
column
or
row
but
we
will
talk
about
missing
data
So
deleting
is
the
worst
that
you
can
come
up
with
Consistency
Again
we'll
talk
about
it
a
little
bit
later
Some
data
was
modified
some
as
not
you
wanted
to
have
data
to
be
written
in
the
same
form
If
it
is
modified
and
all
should
be
if
it
is
not
that
all
should
be
missing
Is
this
data
date
So
if
you
collected
data
or
and
if
you're
stating
that
this
is
the
current
number
for
something
then
you
want
to
make
sure
that
this
current
number
is
not
dated
2010
Okay
Now
can
you
trust
the
data
You
can
talk
a
lot
about
it
but
right
now
it's
just
a
simple
question
at
this
point
If
you
see
that
someone
entered
this
data
just
because
I
promised
several
extra
points
for
completing
this
assignment
Then
District
gardens
If
you
think
that
somebody
deliberately
twisted
data
deliberately
disregard
and
so
on
If
you
can
tell
a
lot
of
times
we
can
not
or
sometimes
there
could
be
mistakes
Or
sometimes
maybe
you
see
data
that
is
coming
from
a
lot
of
different
sources
And
you
know
that
this
particular
source
cannot
be
trusted
So
I'll
go
to
disregard
the
standard
When
you
Google
wanted
to
to
find
answers
to
your
questions
I'm
sure
that
you
look
at
the
sources
that
this
information
is
coming
from
And
you
already
know
some
sources
that
you
always
trust
In
some
sources
that
you
think
okay
There
could
be
read
they
answer
could
be
wrong
wider
with
my
time
And
so
the
same
is
going
to
happen
here
Let's
if
you
know
that
you
can
trust
or
you
should
not
be
trusted
source
Sometimes
you
may
be
unaware
and
use
data
that
is
incorrect
and
all
your
study
is
going
to
come
out
to
indirect
band
So
you
may
get
very
upset
institution
when
you
publish
something
And
then
somebody
will
point
out
that
you
know
you
shouldn't
have
trusted
The
last
question
can
you
understand
can
you
interpret
the
data
Or
for
example
this
metadata
which
is
data
about
your
data
the
explanation
may
be
is
missing
and
you
have
no
idea
how
this
numbers
You
have
the
name
of
the
attribute
but
you
can't
make
sense
of
this
numbers
So
don't
try
to
analyze
something
that
you
don't
understand
Or
maybe
this
category
is
talking
about
some
attribute
Maybe
it
is
related
to
the
subject
that
you
did
not
understand
Maybe
this
reliable
data
may
be
it
is
cleaned
data
complete
data
and
so
on
But
you
don't
understand
what
it
means
Read
about
it
Ask
professional
ask
someone
who
understands
Don't
don't
touch
data
that
you
don't
have
clue
above
Because
you
may
end
up
in
a
very
funny
story
Berkeley's
withdrawn
Okay
right
here
I've
put
some
studies
or
some
ideas
about
data
clean
interests
for
emission
reduction
can
look
later
but
we
will
be
covering
them
Data
cleaning
We
start
with
data
cleaning
You
will
see
this
words
use
later
on
Sometimes
they're
used
interchangeably
if
you
want
you
can
look
into
it
more
Data
Munging
or
some
are
st
one
dream
I've
heard
monk
and
more
and
data
wrangling
So
these
are
slightly
difference
a
different
to
when
not
going
to
talk
much
about
it
He
will
if
you
continue
with
Data
Science
if
you're
good
to
go
to
to
data
science
but
just
understand
they're
dealing
with
data
processing
It
is
some
sort
of
cleaning
and
transforming
data
Alright
Data
cleaning
And
we'll
talk
about
filling
in
missing
values
smoking
noisy
data
Some
outliers
inconsistency
Incomplete
incomplete
data
There
are
no
attribute
values
There
are
no
some
attributes
that
you
are
interested
in
In
any
case
you
have
data
that
is
missing
What
are
you
going
to
do
if
you
have
some
incomplete
data
What
do
you
think
What
do
you
think
You
ideas
you'd
have
incomplete
data
The
data
is
complete
without
those
points
that
have
missing
data
you
just
run
the
analysis
that
requires
the
missing
data
ignore
ignore
ignore
the
whole
rows
right
One
way
of
dealing
with
Ignore
the
whole
row
if
you
have
a
lot
of
missing
data
You
don't
want
to
do
that
So
you
see
for
example
a
student
if
you
collect
the
data
from
let's
say
10
thousand
students
you
see
maybe
five
or
seven
of
them
didn't
answer
half
of
the
questions
You
may
say
Well
maybe
are
just
disregard
the
surveys
from
those
students
It's
possible
it's
possible
you
don't
want
to
to
live
the
whole
attribute
though
For
the
rows
It's
possible
for
the
columns
or
for
the
participants
for
the
entries
It's
possible
for
the
attributes
For
the
variables
You
don't
want
to
delete
the
whole
variable
the
whole
column
In
this
case
download
speed
because
some
data
is
missing
Now
this
is
the
last
thing
that
you're
going
to
do
Delete
something
from
your
data
set
And
you
will
answer
the
question
why
in
a
minute
Now
sometimes
you
may
just
want
to
fill
the
missing
values
with
the
average
right
Sure
Interested
You
may
want
to
show
with
the
average
Yeah
Possible
Possible
You
may
want
to
fill
it
with
the
average
So
right
now
we
have
two
ideas
First
delete
and
second
Replace
with
an
average
or
just
include
an
average
right
here
instead
of
the
missing
data
It's
just
it's
not
just
for
the
missing
data
For
example
the
download
speed
right
here
by
mistake
we
had
minus
20
It's
certainly
wrong
data
right
You
don't
expect
minus
20
to
be
here
Although
some
time
was
between
t
I'm
not
sure
But
in
any
case
it's
not
just
when
it's
empty
unique
to
screen
And
see
EBITDA
belongs
to
the
range
that
you
expect
it
to
below
Note
so
you
may
fill
it
in
with
some
value
Now
why
wouldn't
you
want
to
delete
this
data
if
some
values
are
missing
Why
wouldn't
you
want
to
just
delete
those
roles
Can
still
be
useful
right
It
can
still
be
useful
but
give
me
an
example
We
will
study
will
go
wrong
It
can
be
useful
but
you'll
be
absolutely
wrong
You
skip
those
Think
about
one
of
the
examples
that
sometimes
Given
in
the
books
there
was
a
study
and
it
was
related
to
the
weight
to
something
about
overweight
people
Overweight
people
tend
not
to
want
to
give
their
weight
So
those
people
who
suite
was
above
a
particular
number
two
where
they
thought
they
were
overweight
A
lot
of
those
people
just
missed
the
values
where
the
question
was
above
their
weight
So
the
remaining
values
where
given
a
normal
weight
most
of
the
missed
values
were
related
to
the
hybrid
That
means
the
study
was
weight-related
And
we
miss
on
all
this
tight
high
values
then
the
study
is
just
not
going
to
include
any
of
the
overweight
people
and
only
people
with
normal
or
below
no
rock
We
too
are
going
to
participate
in
this
study
The
whole
analysis
is
going
to
be
skewed
What
else
may
happen
if
you
delete
missing
values
Or
when
it
may
happen
It
may
happen
when
the
question
for
example
you
can
come
up
with
your
own
examples
When
the
question
is
asked
wrong
For
example
your
driver's
license
And
Eve
viscometers
and
listen
maybe
they're
missing
for
a
reason
Maybe
person
does
not
have
a
license
So
we
are
going
to
remove
this
rows
just
because
the
person
does
not
have
drivers
nurses
Always
when
you
ask
this
questions
and
give
some
opportunity
for
the
people
that
cannot
answer
this
question
with
what
you're
asked
to
somehow
answer
So
you
can
always
include
an
I
don't
have
it
Or
you
can
look
at
the
Age
and
infirm
So
this
missing
values
can
tell
you
more
than
those
values
that
are
there
and
so
forth
So
a
lot
of
times
those
missing
values
are
not
there
for
a
reason
Analyze
first
Why
aren't
they
there
One
question
And
this
is
the
data
about
that
houses
the
prices
and
their
location
their
price
number
of
bedrooms
bathrooms
size
And
so
what
end
price
right
here
First
idea
was
to
replace
a
missing
value
with
some
average
or
mean
which
is
a
good
idea
And
a
lot
of
times
it
is
used
What
is
going
to
happen
if
it
is
used
here
In
this
data
set
for
the
price
It's
not
reasonable
to
do
SLA
Correct
Because
but
the
value
of
the
house
depends
on
many
parameters
And
if
you
have
the
highest
values
house
valued
house
whose
price
is
missing
and
you
are
going
to
just
simply
replace
it
with
an
average
It's
going
to
mislead
And
several
of
those
you're
going
to
do
it's
going
to
be
very
misleading
So
in
this
case
what
you
may
want
to
do
is
to
cluster
to
cluster
your
data
to
group
your
data
and
have
similar
houses
similar
valued
houses
to
be
in
a
particular
group
In
this
case
you
can
represent
the
missing
value
with
the
average
for
that
group
With
the
average
for
that
particular
group
Somebody
mentioned
to
You
can
use
a
regression
You
can
use
regression
to
fit
the
value
We're
going
to
work
on
it
later
on
It's
going
to
be
in
a
couple
of
weeks
And
at
that
point
you
will
understand
what
it
means
Right
now
let
me
show
you
just
show
you
a
picture
for
example
So
for
example
you
have
an
idea
about
a
lot
of
data
points
You
want
to
find
this
red
light
red
line
We
will
learn
how
to
do
it
And
if
you
have
missing
values
now
it's
easy
I
have
a
missing
value
For
example
For
example
this
is
I
don't
know
what
it
can
represent
That
can't
be
negative
Let's
say
some
coordinates
It's
the
easiest
that
I
can
think
about
You
have
a
coordinate
X
or
you
know
who
the
status
come
from
and
but
you
don't
have
white
So
you
just
look
at
your
line
of
best
fit
and
you'll
find
the
point
on
the
line
which
will
tell
you
what
y
is
You
know
what
x
is
you
will
know
what
is
The
line
of
best
fit
is
a
great
tool
and
sometimes
it
is
used
for
that
purpose
to
replace
the
missing
data
However
it
is
sometimes
tedious
to
do
so
If
you
have
a
lot
of
data
if
you
have
a
lot
of
parameters
So
it
depends
on
how
important
it
is
for
you
know
noise
What
do
we
do
with
noise
Suppose
you
have
a
random
error
variance
in
a
measured
variable
Of
course
you
would
want
to
access
this
values
If
it
is
a
huge
data
set
it's
very
difficult
to
do
it
manually
If
it
is
a
small
data
set
maybe
you
want
to
do
manually
In
either
case
do
not
just
assume
that
those
are
because
of
some
error
Maybe
not
maybe
not
and
you
will
miss
a
very
important
information
Okay
Now
never
just
drunk
data
Or
disregard
outliers
for
your
study
You
may
want
to
remove
the
outliers
but
first
analyze
them
before
you
or
how
do
we
deal
with
noisy
data
The
easiest
is
been
thinning
You
have
lots
of
data
That
is
for
example
that
is
all
over
the
place
everywhere
Suppose
there
is
a
time
series
and
you
want
to
see
what's
happening
every
day
And
if
you
remember
I
was
showing
this
data
uncovered
So
in
that
case
each
data
point
was
showing
how
many
cases
there
were
diagnosed
was
covered
in
date
So
you
may
have
some
days
for
example
Saturday
and
Sunday
where
the
numbers
are
going
lower
Does
it
mean
that
the
numbers
went
down
Note
it
doesn't
Maybe
they
were
just
thought
recorded
reported
on
Saturday
Sunday
because
it
was
a
weekend
And
then
on
Monday
and
Tuesday
we're
searching
Is
it
because
a
lot
of
a
lot
more
people
who
are
diagnostic
Do
we
have
another
case
when
the
numbers
of
rapidly
growing
up
Not
necessarily
Maybe
it's
because
on
Saturday
Sunday
they
were
undiagnosed
and
then
on
Monday
Tuesday
they
came
out
to
be
the
numbers
came
out
to
be
hard
because
of
that
So
in
that
case
they
were
smoothing
the
data
by
using
a
day
average
70
average
which
means
that
at
each
data
point
they
looked
back
seven
days
back
And
the
curve
was
smooth
and
your
data
in
what's
showing
the
trend
In
that
case
In
case
of
Benin
you
will
say
alright
instead
of
looking
at
data
Monday
Monday
Tuesday
Wednesday
Thursday
I
will
just
put
the
whole
week
in
one
bin
And
that
is
going
to
be
a
bit
average
for
the
week
And
that
is
going
to
be
my
next
week
I'm
sorry
it
must
be
the
same
And
that
is
going
to
be
my
next
week
is
the
average
was
higher
And
then
it's
going
to
be
my
next
week
And
another
week
and
another
Okay
This
is
binning
How
can
you
decide
on
what
is
going
to
be
the
value
that
represents
the
bin
Well
it
can
always
be
the
average
It
can
be
it
can
be
the
first
day
represents
or
the
last
date
in
our
COBIT
case
represents
the
bin
So
there
are
different
ways
of
doing
so
but
Benin
is
very
simple
and
it's
very
helpful
very
simple
and
very
helpful
What
you
do
you
have
data
for
example
right
here
This
is
priced
in
dollars
unique
to
ordered
first
and
then
you
partition
it
into
groups
of
the
same
size
Alright
And
the
last
one
whenever
is
left
inside
our
particular
group
you
decide
what
to
do
So
you
can
smooth
it
You
can
smooth
it
by
means
is
it's
done
here
You
can
smooth
it
by
being
boundaries
Have
been
founders
either
in
our
covert
case
the
first
day
is
going
to
be
the
height
of
it
or
the
last
day
whatever
you
choose
or
the
mean
And
we
looked
already
at
dealing
with
noise
right
here
by
using
the
regression
So
I'm
sorry
you're
dealing
with
missing
data
by
using
the
regression
but
also
we
can
deal
with
noise
the
same
way
by
using
regression
Likewise
you
can
use
clustering
We
already
mentioned
it
when
we
were
talking
about
to
our
missing
data
So
if
you
know
if
you
defined
the
groups
that
your
data
belongs
to
for
example
these
are
low
The
houses
that
are
on
the
lower
side
price-wise
medium
and
the
largest
value
Well
should
we
probably
that's
the
largest
The
medium
and
the
lower
Something
like
that
If
you
can
cluster
or
partition
into
groups
And
then
you
have
some
outliers
If
you
think
that
for
those
outliers
the
price
needed
just
mistakenly
recorded
the
way
it
was
And
all
the
parameters
all
the
other
attributes
for
those
houses
are
similar
then
you
may
want
to
include
it
into
the
same
cluster
Alright
I'm
going
to
stop
right
now
